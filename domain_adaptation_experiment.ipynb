{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- utils.py content ---\n",
    "def save_gan_images(images: torch.Tensor, epoch: int, n_classes: int = 10, output_dir: str = \"outputs/gan_images\") -> None:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    grid = vutils.make_grid(images, nrow=n_classes, normalize=True)\n",
    "    vutils.save_image(grid, os.path.join(output_dir, f\"epoch_{epoch:03d}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- models.py content ---\n",
    "def weights_init(m: nn.Module) -> None:\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Classifier, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5, 1, 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 5, 1, 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return self.main(input)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz: int = 100, ngf: int = 64, nc: int = 1, n_classes: int = 10) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(n_classes, nz)\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz * 2, ngf * 4, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([noise, c], 1)\n",
    "        x = x.view(x.size(0), -1, 1, 1)\n",
    "        return self.main(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc: int = 1, ndf: int = 64, n_classes: int = 10) -> None:\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_embedding = nn.Embedding(n_classes, 32 * 32)\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc + 1, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        label_embedding_reshaped = self.label_embedding(labels).view(labels.size(0), 1, 32, 32)\n",
    "        d_in = torch.cat((img, label_embedding_reshaped), 1)\n",
    "        return self.main(d_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- data_loader.py content ---\n",
    "class ToTensorLong:\n",
    "    def __call__(self, y):\n",
    "        return torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "def get_dataloaders(\n",
    "    low_resource_size: int, batch_size: int = 64\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader, DataLoader]:\n",
    "    target_transform = ToTensorLong()\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(32),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    mnist_train = datasets.MNIST(\n",
    "        root=\"./data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "        target_transform=target_transform,\n",
    "    )\n",
    "    mnist_train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    svhn_train = datasets.SVHN(\n",
    "        root=\"./data\",\n",
    "        split=\"train\",\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "        target_transform=target_transform,\n",
    "    )\n",
    "    svhn_test = datasets.SVHN(\n",
    "        root=\"./data\",\n",
    "        split=\"test\",\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "        target_transform=target_transform,\n",
    "    )\n",
    "\n",
    "    svhn_test_loader = DataLoader(svhn_test, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    targets = np.array(svhn_train.labels)\n",
    "    indices = []\n",
    "    samples_per_class = max(1, int(low_resource_size / 10)) if low_resource_size > 0 else 0\n",
    "\n",
    "    if low_resource_size > 0:\n",
    "        for i in range(10):\n",
    "            class_indices = np.where(targets == i)[0]\n",
    "            num_to_sample = min(samples_per_class, len(class_indices))\n",
    "            indices.extend(np.random.choice(class_indices, num_to_sample, replace=False))\n",
    "\n",
    "    svhn_low_resource_subset = Subset(svhn_train, indices)\n",
    "    svhn_low_resource_loader = DataLoader(\n",
    "        svhn_low_resource_subset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    "    )\n",
    "\n",
    "    transform_augmented = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(32),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,)),\n",
    "        ]\n",
    "    )\n",
    "    svhn_train_augmented = datasets.SVHN(\n",
    "        root=\"./data\",\n",
    "        split=\"train\",\n",
    "        download=True,\n",
    "        transform=transform_augmented,\n",
    "        target_transform=target_transform,\n",
    "    )\n",
    "    svhn_augmented_subset = Subset(svhn_train_augmented, indices)\n",
    "    svhn_trad_aug_loader = DataLoader(\n",
    "        svhn_augmented_subset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        mnist_train_loader,\n",
    "        svhn_low_resource_loader,\n",
    "        svhn_trad_aug_loader,\n",
    "        svhn_test_loader,\n",
    "    )\n",
    "\n",
    "def get_gan_augmented_loader(\n",
    "    svhn_low_resource_loader: DataLoader,\n",
    "    generated_images: torch.Tensor,\n",
    "    generated_labels: torch.Tensor,\n",
    "    batch_size: int = 64,\n",
    ") -> DataLoader:\n",
    "    generated_labels = generated_labels.to(dtype=torch.long)\n",
    "    generated_dataset = TensorDataset(generated_images, generated_labels)\n",
    "    combined_dataset = ConcatDataset(\n",
    "        [svhn_low_resource_loader.dataset, generated_dataset]\n",
    "    )\n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    return combined_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- train_gan.py content ---\n",
    "def train_gan(\n",
    "    n_samples: int,\n",
    "    num_epochs: int = 100,\n",
    "    nz: int = 100,\n",
    "    lr: float = 0.0002,\n",
    "    beta1: float = 0.5,\n",
    "    batch_size: int = 64,\n",
    "    ngf: int = 64,\n",
    "    ndf: int = 64,\n",
    "    nc: int = 1,\n",
    "    n_classes: int = 10,\n",
    "    output_dir: str = \"outputs\",\n",
    ") -> None:\n",
    "    print(f\"train_gan: output_dir={output_dir}\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Starting GAN training on device: {device}\")\n",
    "\n",
    "    log_dir = os.path.join(\n",
    "        output_dir, \"tensorboard_logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + f\"_GAN_N{n_samples}\"\n",
    "    )\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    print(f\"TensorBoard logs for GAN training will be saved to: {log_dir}\")\n",
    "\n",
    "    _, svhn_low_resource_loader, _, _ = get_dataloaders(\n",
    "        low_resource_size=n_samples, batch_size=batch_size\n",
    "    )\n",
    "    if len(svhn_low_resource_loader.dataset) == 0:\n",
    "        print(f\"Warning: No low-resource SVHN samples available for n_samples={n_samples}. GAN training skipped.\")\n",
    "        return\n",
    "\n",
    "    netG = Generator(nz=nz, ngf=ngf, nc=nc, n_classes=n_classes).to(device)\n",
    "    netD = Discriminator(nc=nc, ndf=ndf, n_classes=n_classes).to(device)\n",
    "\n",
    "    netG.apply(weights_init)\n",
    "    netD.apply(weights_init)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    fixed_noise = torch.randn(100, nz, device=device)\n",
    "    fixed_labels = torch.arange(0, n_classes, device=device).repeat(10)\n",
    "\n",
    "    real_batch = next(iter(svhn_low_resource_loader))\n",
    "    writer.add_image(\"GAN/Real Images\", vutils.make_grid(real_batch[0][:100], nrow=10, normalize=True), 0)\n",
    "\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "    print(f\"Starting GAN training with {num_epochs} epochs on {n_samples} SVHN samples...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_images, real_labels) in enumerate(svhn_low_resource_loader):\n",
    "            real_images = real_images.to(device)\n",
    "            real_labels = real_labels.to(device)\n",
    "            b_size = real_images.size(0)\n",
    "\n",
    "            netD.zero_grad()\n",
    "            label_real = torch.full((b_size,), 1.0, dtype=torch.float, device=device)\n",
    "            output = netD(real_images, real_labels).view(-1)\n",
    "            errD_real = criterion(output, label_real)\n",
    "            errD_real.backward()\n",
    "\n",
    "            noise = torch.randn(b_size, nz, device=device)\n",
    "            fake_labels = torch.randint(0, n_classes, (b_size,), device=device)\n",
    "            fake_images = netG(noise, fake_labels)\n",
    "            label_fake = torch.full((b_size,), 0.0, dtype=torch.float, device=device)\n",
    "            output = netD(fake_images.detach(), fake_labels).view(-1)\n",
    "            errD_fake = criterion(output, label_fake)\n",
    "            errD_fake.backward()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "            netG.zero_grad()\n",
    "            output = netD(fake_images, fake_labels).view(-1)\n",
    "            errG = criterion(\n",
    "                output, label_real\n",
    "            )\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "            f\"Loss_D: {errD.item():.4f} \"\n",
    "            f\"Loss_G: {errG.item():.4f}\"\n",
    "        )\n",
    "        writer.add_scalar(\"GAN/Loss_D\", errD.item(), epoch)\n",
    "        writer.add_scalar(\"GAN/Loss_G\", errG.item(), epoch)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or (epoch + 1) == num_epochs:\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise, fixed_labels).detach().cpu()\n",
    "            gan_images_dir = os.path.join(output_dir, \"gan_images\")\n",
    "            os.makedirs(gan_images_dir, exist_ok=True)\n",
    "            save_gan_images(fake, epoch + 1, n_classes=n_classes, output_dir=gan_images_dir)\n",
    "            print(f\"Saved generated images for epoch {epoch+1}.\")\n",
    "            img_grid = vutils.make_grid(fake, nrow=n_classes, normalize=True)\n",
    "            writer.add_image(\"GAN/Generated Images\", img_grid, epoch)\n",
    "\n",
    "    gan_checkpoints_dir = os.path.join(output_dir, \"checkpoints\")\n",
    "    os.makedirs(gan_checkpoints_dir, exist_ok=True)\n",
    "    gan_model_path = os.path.join(gan_checkpoints_dir, f\"gan_generator_n{n_samples}.pth\")\n",
    "    torch.save(netG.state_dict(), gan_model_path)\n",
    "    print(f\"Finished GAN Training. Generator model saved to {gan_model_path}\")\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"TensorBoard writer closed. View logs with: tensorboard --logdir {os.path.dirname(log_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- main_experiment.py content ---\n",
    "def train_classifier(\n",
    "    model: nn.Module,\n",
    "    train_loader: 'DataLoader',\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    current_epoch: int,\n",
    "    total_epochs: int,\n",
    ") -> None:\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"  Epoch {current_epoch}/{total_epochs} Batch {batch_idx}/{len(train_loader)} Loss: {loss.item():.4f}\")\n",
    "\n",
    "def evaluate_classifier(\n",
    "    model: nn.Module, test_loader: 'DataLoader', device: torch.device\n",
    ") -> tuple[float, str]:\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    report = classification_report(all_targets, all_preds, digits=4)\n",
    "    return accuracy, report\n",
    "\n",
    "def _pretrain_classifier(\n",
    "    classifier: nn.Module,\n",
    "    mnist_loader: 'DataLoader',\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    epochs: int,\n",
    "    output_dir: str,\n",
    ") -> nn.Module:\n",
    "    mnist_model_path = os.path.join(output_dir, \"checkpoints\", \"classifier_mnist_pretrained.pth\")\n",
    "\n",
    "    if not os.path.exists(mnist_model_path):\n",
    "        print(f\"Pre-training classifier on MNIST for {epochs} epochs...\")\n",
    "        for epoch in range(epochs):\n",
    "            train_classifier(classifier, mnist_loader, optimizer, criterion, device, epoch + 1, epochs)\n",
    "            print(f\"MNIST Pre-train Epoch {epoch+1}/{epochs} complete.\")\n",
    "        os.makedirs(os.path.dirname(mnist_model_path), exist_ok=True)\n",
    "        torch.save(classifier.state_dict(), mnist_model_path)\n",
    "        print(f\"Pre-trained MNIST classifier saved to {mnist_model_path}\")\n",
    "    else:\n",
    "        print(f\"Loading pre-trained MNIST classifier from {mnist_model_path}...\")\n",
    "        classifier.load_state_dict(torch.load(mnist_model_path, map_location=device))\n",
    "        print(\"Pre-trained MNIST classifier loaded.\")\n",
    "    return classifier\n",
    "\n",
    "def _run_source_only_scenario(\n",
    "    classifier: nn.Module, device: torch.device, output_dir: str\n",
    ") -> nn.Module:\n",
    "    print(\"Scenario: Source Only - Evaluating MNIST pre-trained model directly on SVHN.\")\n",
    "    model_save_path = os.path.join(output_dir, \"checkpoints\", \"classifier_source_only.pth\")\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    torch.save(classifier.state_dict(), model_save_path)\n",
    "    print(f\"Source-only classifier model saved to {model_save_path}\")\n",
    "    return classifier\n",
    "\n",
    "def _run_fine_tune_scenario(\n",
    "    classifier: nn.Module,\n",
    "    svhn_low_loader: 'DataLoader',\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    epochs: int,\n",
    "    output_dir: str,\n",
    ") -> nn.Module:\n",
    "    print(f\"Scenario: Fine-tuning on low-resource SVHN data for {epochs} epochs...\")\n",
    "    for epoch in range(epochs):\n",
    "        train_classifier(classifier, svhn_low_loader, optimizer, criterion, device, epoch + 1, epochs)\n",
    "        print(f\"SVHN Fine-tune Epoch {epoch+1}/{epochs} complete.\")\n",
    "    model_save_path = os.path.join(output_dir, \"checkpoints\", \"classifier_fine_tune.pth\")\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    torch.save(classifier.state_dict(), model_save_path)\n",
    "    print(f\"Fine-tuned classifier model saved to {model_save_path}\")\n",
    "    return classifier\n",
    "\n",
    "def _run_traditional_aug_scenario(\n",
    "    classifier: nn.Module,\n",
    "    svhn_trad_aug_loader: 'DataLoader',\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    epochs: int,\n",
    "    output_dir: str,\n",
    ") -> nn.Module:\n",
    "    print(f\"Scenario: Fine-tuning on traditionally augmented low-resource SVHN data for {epochs} epochs...\")\n",
    "    for epoch in range(epochs):\n",
    "        train_classifier(classifier, svhn_trad_aug_loader, optimizer, criterion, device, epoch + 1, epochs)\n",
    "        print(f\"SVHN Traditional Aug Epoch {epoch+1}/{epochs} complete.\")\n",
    "    model_save_path = os.path.join(output_dir, \"checkpoints\", \"classifier_traditional_aug.pth\")\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    torch.save(classifier.state_dict(), model_save_path)\n",
    "    print(f\"Traditional augmented classifier model saved to {model_save_path}\")\n",
    "    return classifier\n",
    "\n",
    "def _run_gan_aug_scenario(\n",
    "    classifier: nn.Module,\n",
    "    svhn_low_loader: 'DataLoader',\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    args: argparse.Namespace,\n",
    "    output_dir: str,\n",
    "    gan_model_base_dir: str = None,\n",
    ") -> nn.Module:\n",
    "    print(f\"  _run_gan_aug_scenario: output_dir={output_dir}, gan_model_base_dir={gan_model_base_dir}\")\n",
    "    print(f\"Scenario: Fine-tuning on GAN-augmented SVHN data for {args.classifier_epochs_finetune} epochs...\")\n",
    "    generator = Generator(nz=args.gan_nz, ngf=args.gan_ngf, nc=args.gan_nc, n_classes=args.gan_n_classes).to(device)\n",
    "    \n",
    "    gan_load_dir = gan_model_base_dir if gan_model_base_dir else output_dir\n",
    "    gan_model_path = os.path.join(gan_load_dir, \"checkpoints\", f\"gan_generator_n{args.n_samples}.pth\")\n",
    "    print(f\"  _run_gan_aug_scenario: gan_load_dir={gan_load_dir}, gan_model_path={gan_model_path}\")\n",
    "    \n",
    "    if not os.path.exists(gan_model_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"GAN generator model for n_samples={args.n_samples} not found at {gan_model_path}. \"\n",
    "            f\"Please ensure the GAN was trained and saved to the correct output directory.\"\n",
    "        )\n",
    "    generator.load_state_dict(torch.load(gan_model_path, map_location=device))\n",
    "    generator.eval()\n",
    "\n",
    "    print(f\"Generating {args.n_synthetic} synthetic images using GAN...\")\n",
    "    noise = torch.randn(args.n_synthetic, args.gan_nz, device=device)\n",
    "    labels = torch.randint(0, args.gan_n_classes, (args.n_synthetic,), device=device)\n",
    "    with torch.no_grad():\n",
    "        synthetic_images = generator(noise, labels).cpu()\n",
    "    print(\"Synthetic images generated.\")\n",
    "\n",
    "    gan_augmented_loader = get_gan_augmented_loader(\n",
    "        svhn_low_loader, synthetic_images, labels.cpu(), batch_size=args.batch_size\n",
    "    )\n",
    "    print(\"GAN-augmented data loader created.\")\n",
    "\n",
    "    for epoch in range(args.classifier_epochs_finetune):\n",
    "        train_classifier(\n",
    "            classifier, gan_augmented_loader, optimizer, criterion, device, epoch + 1, args.classifier_epochs_finetune\n",
    "        )\n",
    "        print(f\"SVHN GAN Aug Epoch {epoch+1}/{args.classifier_epochs_finetune} complete.\")\n",
    "    model_save_path = os.path.join(output_dir, \"checkpoints\", \"classifier_gan_aug.pth\")\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    torch.save(classifier.state_dict(), model_save_path)\n",
    "    print(f\"GAN-augmented classifier model saved to {model_save_path}\")\n",
    "    return classifier\n",
    "\n",
    "def run_experiment(\n",
    "    args: argparse.Namespace,\n",
    "    gan_model_base_dir: str = None,\n",
    ") -> None:\n",
    "    print(f\"run_experiment: args.output_dir={args.output_dir}, gan_model_base_dir={gan_model_base_dir}\")\n",
    "    log_dir = os.path.join(\n",
    "        args.output_dir, \"tensorboard_logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + f\"_{args.scenario}_N{args.n_samples}\"\n",
    "    )\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\n===== RUNNING SCENARIO: {args.scenario.upper()} with N={args.n_samples} on {device} =====\")\n",
    "\n",
    "    mnist_loader, svhn_low_loader, svhn_trad_aug_loader, svhn_test_loader = (\n",
    "        get_dataloaders(low_resource_size=args.n_samples, batch_size=args.batch_size)\n",
    "    )\n",
    "\n",
    "    classifier = Classifier().to(device)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=args.classifier_lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    classifier = _pretrain_classifier(\n",
    "        classifier, mnist_loader, optimizer, criterion, device, args.classifier_epochs_mnist, args.output_dir\n",
    "    )\n",
    "\n",
    "    if args.scenario == \"source_only\":\n",
    "        classifier = _run_source_only_scenario(classifier, device, args.output_dir)\n",
    "    elif args.scenario == \"fine_tune\":\n",
    "        classifier = _run_fine_tune_scenario(\n",
    "            classifier, svhn_low_loader, optimizer, criterion, device, args.classifier_epochs_finetune, args.output_dir\n",
    "        )\n",
    "    elif args.scenario == \"traditional_aug\":\n",
    "        classifier = _run_traditional_aug_scenario(\n",
    "            classifier, svhn_trad_aug_loader, optimizer, criterion, device, args.classifier_epochs_finetune, args.output_dir\n",
    "        )\n",
    "    elif args.scenario == \"gan_aug\":\n",
    "        print(f\"run_experiment: Calling _run_gan_aug_scenario with output_dir={args.output_dir}, gan_model_base_dir={gan_model_base_dir}\")\n",
    "        classifier = _run_gan_aug_scenario(\n",
    "            classifier, svhn_low_loader, optimizer, criterion, device, args, args.output_dir, gan_model_base_dir\n",
    "        )\n",
    "\n",
    "    print(\"\\n--- Performing Final Evaluation on SVHN Test Set ---\")\n",
    "    accuracy, report = evaluate_classifier(classifier, svhn_test_loader, device)\n",
    "    print(f\"\\n--- Results for Scenario: {args.scenario.upper()} (N={args.n_samples}) ---\")\n",
    "    print(f\"Final Accuracy on SVHN Test Set: {accuracy * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    writer.add_scalar(\"Final Accuracy/SVHN\", accuracy, 0)\n",
    "\n",
    "    results = {\n",
    "        \"scenario\": args.scenario,\n",
    "        \"n_samples\": args.n_samples,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"classifier_lr\": args.classifier_lr,\n",
    "        \"classifier_epochs_mnist\": args.classifier_epochs_mnist,\n",
    "        \"classifier_epochs_finetune\": args.classifier_epochs_finetune,\n",
    "        \"gan_nz\": args.gan_nz,\n",
    "        \"gan_ngf\": args.gan_ngf,\n",
    "        \"gan_ndf\": args.gan_ndf,\n",
    "        \"gan_nc\": args.gan_nc,\n",
    "        \"gan_n_classes\": args.gan_n_classes,\n",
    "        \"n_synthetic\": args.n_synthetic,\n",
    "        \"final_accuracy\": accuracy,\n",
    "        \"classification_report\": report,\n",
    "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "        \"log_dir\": log_dir,\n",
    "    }\n",
    "\n",
    "    results_dir = os.path.join(args.output_dir, \"results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    results_filename = f\"{args.scenario}_N{args.n_samples}_results.json\"\n",
    "    results_path = os.path.join(results_dir, results_filename)\n",
    "\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"Experiment results saved to: {results_path}\")\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"TensorBoard writer closed. View logs with: tensorboard --logdir {os.path.dirname(log_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing for experiments with N_samples = 10 ---\n",
      "Experiment Loop: base_experiment_run_dir=./experiments\\20251022-135817_N10\n",
      "Experiment Loop: Calling train_gan with output_dir=./experiments\\20251022-135817_N10\n",
      "train_gan: output_dir=./experiments\\20251022-135817_N10\n",
      "Starting GAN training on device: cuda:0\n",
      "TensorBoard logs for GAN training will be saved to: ./experiments\\20251022-135817_N10\\tensorboard_logs\\20251022-135817_GAN_N10\n",
      "Starting GAN training with 100 epochs on 10 SVHN samples...\n",
      "Epoch [1/100] Loss_D: 1.4078 Loss_G: 2.1752\n",
      "Epoch [2/100] Loss_D: 2.3618 Loss_G: 1.5000\n",
      "Epoch [3/100] Loss_D: 1.5394 Loss_G: 1.6555\n",
      "Epoch [4/100] Loss_D: 1.0215 Loss_G: 1.8654\n",
      "Epoch [5/100] Loss_D: 1.2114 Loss_G: 1.3626\n",
      "Epoch [6/100] Loss_D: 1.2608 Loss_G: 1.7763\n",
      "Epoch [7/100] Loss_D: 1.0519 Loss_G: 2.3866\n",
      "Epoch [8/100] Loss_D: 1.1307 Loss_G: 2.1906\n",
      "Epoch [9/100] Loss_D: 1.1952 Loss_G: 1.8247\n",
      "Epoch [10/100] Loss_D: 0.7428 Loss_G: 1.9096\n",
      "Saved generated images for epoch 10.\n",
      "Epoch [11/100] Loss_D: 0.7783 Loss_G: 1.9274\n",
      "Epoch [12/100] Loss_D: 0.6543 Loss_G: 2.0445\n",
      "Epoch [13/100] Loss_D: 0.7255 Loss_G: 2.4393\n",
      "Epoch [14/100] Loss_D: 0.6340 Loss_G: 2.5860\n",
      "Epoch [15/100] Loss_D: 0.6431 Loss_G: 2.4166\n",
      "Epoch [16/100] Loss_D: 0.7414 Loss_G: 2.4296\n",
      "Epoch [17/100] Loss_D: 0.5560 Loss_G: 2.4645\n",
      "Epoch [18/100] Loss_D: 0.4254 Loss_G: 2.4103\n",
      "Epoch [19/100] Loss_D: 0.3980 Loss_G: 2.3366\n",
      "Epoch [20/100] Loss_D: 0.4586 Loss_G: 2.4422\n",
      "Saved generated images for epoch 20.\n",
      "Epoch [21/100] Loss_D: 0.4663 Loss_G: 2.5297\n",
      "Epoch [22/100] Loss_D: 0.3293 Loss_G: 2.8153\n",
      "Epoch [23/100] Loss_D: 0.3990 Loss_G: 2.4405\n",
      "Epoch [24/100] Loss_D: 0.5601 Loss_G: 2.7200\n",
      "Epoch [25/100] Loss_D: 0.6185 Loss_G: 2.6707\n",
      "Epoch [26/100] Loss_D: 0.6806 Loss_G: 2.2233\n",
      "Epoch [27/100] Loss_D: 0.6139 Loss_G: 2.6733\n",
      "Epoch [28/100] Loss_D: 0.8429 Loss_G: 2.3409\n",
      "Epoch [29/100] Loss_D: 1.0933 Loss_G: 2.9441\n",
      "Epoch [30/100] Loss_D: 1.0258 Loss_G: 2.3131\n",
      "Saved generated images for epoch 30.\n",
      "Epoch [31/100] Loss_D: 1.0076 Loss_G: 1.9874\n",
      "Epoch [32/100] Loss_D: 0.8559 Loss_G: 2.2670\n",
      "Epoch [33/100] Loss_D: 1.1376 Loss_G: 2.4983\n",
      "Epoch [34/100] Loss_D: 0.7576 Loss_G: 2.7368\n",
      "Epoch [35/100] Loss_D: 0.5493 Loss_G: 2.4512\n",
      "Epoch [36/100] Loss_D: 1.7993 Loss_G: 2.7765\n",
      "Epoch [37/100] Loss_D: 1.6307 Loss_G: 2.2736\n",
      "Epoch [38/100] Loss_D: 1.1783 Loss_G: 1.8123\n",
      "Epoch [39/100] Loss_D: 0.6928 Loss_G: 1.3553\n",
      "Epoch [40/100] Loss_D: 1.0497 Loss_G: 1.5558\n",
      "Saved generated images for epoch 40.\n",
      "Epoch [41/100] Loss_D: 0.5963 Loss_G: 2.6151\n",
      "Epoch [42/100] Loss_D: 0.9749 Loss_G: 4.1362\n",
      "Epoch [43/100] Loss_D: 1.0211 Loss_G: 2.8275\n",
      "Epoch [44/100] Loss_D: 0.8837 Loss_G: 1.6654\n",
      "Epoch [45/100] Loss_D: 1.2381 Loss_G: 1.8628\n",
      "Epoch [46/100] Loss_D: 0.9544 Loss_G: 2.1452\n",
      "Epoch [47/100] Loss_D: 0.9594 Loss_G: 2.4334\n",
      "Epoch [48/100] Loss_D: 0.8964 Loss_G: 1.5442\n",
      "Epoch [49/100] Loss_D: 0.9791 Loss_G: 1.6231\n",
      "Epoch [50/100] Loss_D: 0.7773 Loss_G: 1.8989\n",
      "Saved generated images for epoch 50.\n",
      "Epoch [51/100] Loss_D: 1.1429 Loss_G: 2.5701\n",
      "Epoch [52/100] Loss_D: 1.1940 Loss_G: 1.6665\n",
      "Epoch [53/100] Loss_D: 0.9871 Loss_G: 1.5260\n",
      "Epoch [54/100] Loss_D: 1.0045 Loss_G: 2.3920\n",
      "Epoch [55/100] Loss_D: 1.1388 Loss_G: 1.9463\n",
      "Epoch [56/100] Loss_D: 0.8401 Loss_G: 1.5906\n",
      "Epoch [57/100] Loss_D: 1.0996 Loss_G: 1.9141\n",
      "Epoch [58/100] Loss_D: 1.2560 Loss_G: 2.1298\n",
      "Epoch [59/100] Loss_D: 0.9274 Loss_G: 1.5508\n",
      "Epoch [60/100] Loss_D: 0.7544 Loss_G: 1.5056\n",
      "Saved generated images for epoch 60.\n",
      "Epoch [61/100] Loss_D: 0.9277 Loss_G: 2.2759\n",
      "Epoch [62/100] Loss_D: 0.8968 Loss_G: 2.4728\n",
      "Epoch [63/100] Loss_D: 1.0143 Loss_G: 1.7004\n",
      "Epoch [64/100] Loss_D: 0.9105 Loss_G: 1.6681\n",
      "Epoch [65/100] Loss_D: 0.5812 Loss_G: 2.0719\n",
      "Epoch [66/100] Loss_D: 1.5568 Loss_G: 2.6634\n",
      "Epoch [67/100] Loss_D: 1.5021 Loss_G: 2.0669\n",
      "Epoch [68/100] Loss_D: 1.2253 Loss_G: 1.7395\n",
      "Epoch [69/100] Loss_D: 1.0292 Loss_G: 1.2691\n",
      "Epoch [70/100] Loss_D: 0.8546 Loss_G: 2.5774\n",
      "Saved generated images for epoch 70.\n",
      "Epoch [71/100] Loss_D: 0.8726 Loss_G: 2.1122\n",
      "Epoch [72/100] Loss_D: 0.6556 Loss_G: 2.1534\n",
      "Epoch [73/100] Loss_D: 1.7323 Loss_G: 3.0806\n",
      "Epoch [74/100] Loss_D: 1.5879 Loss_G: 1.9399\n",
      "Epoch [75/100] Loss_D: 1.2888 Loss_G: 1.9027\n",
      "Epoch [76/100] Loss_D: 1.2514 Loss_G: 1.1163\n",
      "Epoch [77/100] Loss_D: 1.1837 Loss_G: 1.4212\n",
      "Epoch [78/100] Loss_D: 1.1688 Loss_G: 1.3021\n",
      "Epoch [79/100] Loss_D: 0.9083 Loss_G: 1.5845\n",
      "Epoch [80/100] Loss_D: 1.0887 Loss_G: 1.7526\n",
      "Saved generated images for epoch 80.\n",
      "Epoch [81/100] Loss_D: 1.0246 Loss_G: 2.0274\n",
      "Epoch [82/100] Loss_D: 1.0304 Loss_G: 1.8079\n",
      "Epoch [83/100] Loss_D: 1.4546 Loss_G: 1.8462\n",
      "Epoch [84/100] Loss_D: 1.1379 Loss_G: 1.5660\n",
      "Epoch [85/100] Loss_D: 1.1546 Loss_G: 2.1267\n",
      "Epoch [86/100] Loss_D: 1.1788 Loss_G: 1.8560\n",
      "Epoch [87/100] Loss_D: 1.4258 Loss_G: 1.8781\n",
      "Epoch [88/100] Loss_D: 1.0098 Loss_G: 1.6531\n",
      "Epoch [89/100] Loss_D: 0.8022 Loss_G: 2.0965\n",
      "Epoch [90/100] Loss_D: 1.5956 Loss_G: 2.1856\n",
      "Saved generated images for epoch 90.\n",
      "Epoch [91/100] Loss_D: 1.0019 Loss_G: 2.1063\n",
      "Epoch [92/100] Loss_D: 1.0882 Loss_G: 2.5873\n",
      "Epoch [93/100] Loss_D: 1.3227 Loss_G: 2.3742\n",
      "Epoch [94/100] Loss_D: 1.0587 Loss_G: 2.0090\n",
      "Epoch [95/100] Loss_D: 1.1975 Loss_G: 1.5004\n",
      "Epoch [96/100] Loss_D: 0.9582 Loss_G: 1.4360\n",
      "Epoch [97/100] Loss_D: 0.6039 Loss_G: 2.1942\n",
      "Epoch [98/100] Loss_D: 1.5931 Loss_G: 1.6983\n",
      "Epoch [99/100] Loss_D: 1.0319 Loss_G: 2.0307\n",
      "Epoch [100/100] Loss_D: 1.1266 Loss_G: 1.7808\n",
      "Saved generated images for epoch 100.\n",
      "Finished GAN Training. Generator model saved to ./experiments\\20251022-135817_N10\\checkpoints\\gan_generator_n10.pth\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-135817_N10\\tensorboard_logs\n",
      "GAN training for n_samples=10 completed.\n",
      "\n",
      "Running experiment for scenario: source_only with N=10\n",
      "Experiment Loop: Calling run_experiment for scenario=source_only, n_samples=10 with args.output_dir=./experiments\\20251022-135817_N10\\exp_source_only_N10, gan_model_base_dir=./experiments\\20251022-135817_N10\n",
      "run_experiment: args.output_dir=./experiments\\20251022-135817_N10\\exp_source_only_N10, gan_model_base_dir=./experiments\\20251022-135817_N10\n",
      "TensorBoard logs will be saved to: ./experiments\\20251022-135817_N10\\exp_source_only_N10\\tensorboard_logs\\20251022-135827_source_only_N10\n",
      "\n",
      "===== RUNNING SCENARIO: SOURCE_ONLY with N=10 on cuda:0 =====\n",
      "Pre-training classifier on MNIST for 5 epochs...\n",
      "  Epoch 1/5 Batch 0/938 Loss: 2.3112\n",
      "  Epoch 1/5 Batch 100/938 Loss: 0.2351\n",
      "  Epoch 1/5 Batch 200/938 Loss: 0.0581\n",
      "  Epoch 1/5 Batch 300/938 Loss: 0.1098\n",
      "  Epoch 1/5 Batch 400/938 Loss: 0.0173\n",
      "  Epoch 1/5 Batch 500/938 Loss: 0.2575\n",
      "  Epoch 1/5 Batch 600/938 Loss: 0.0639\n",
      "  Epoch 1/5 Batch 700/938 Loss: 0.0141\n",
      "  Epoch 1/5 Batch 800/938 Loss: 0.0601\n",
      "  Epoch 1/5 Batch 900/938 Loss: 0.0167\n",
      "MNIST Pre-train Epoch 1/5 complete.\n",
      "  Epoch 2/5 Batch 0/938 Loss: 0.0057\n",
      "  Epoch 2/5 Batch 100/938 Loss: 0.0699\n",
      "  Epoch 2/5 Batch 200/938 Loss: 0.0206\n",
      "  Epoch 2/5 Batch 300/938 Loss: 0.0462\n",
      "  Epoch 2/5 Batch 400/938 Loss: 0.0268\n",
      "  Epoch 2/5 Batch 500/938 Loss: 0.0106\n",
      "  Epoch 2/5 Batch 600/938 Loss: 0.0448\n",
      "  Epoch 2/5 Batch 700/938 Loss: 0.0140\n",
      "  Epoch 2/5 Batch 800/938 Loss: 0.0189\n",
      "  Epoch 2/5 Batch 900/938 Loss: 0.0012\n",
      "MNIST Pre-train Epoch 2/5 complete.\n",
      "  Epoch 3/5 Batch 0/938 Loss: 0.0010\n",
      "  Epoch 3/5 Batch 100/938 Loss: 0.0036\n",
      "  Epoch 3/5 Batch 200/938 Loss: 0.2111\n",
      "  Epoch 3/5 Batch 300/938 Loss: 0.2874\n",
      "  Epoch 3/5 Batch 400/938 Loss: 0.0025\n",
      "  Epoch 3/5 Batch 500/938 Loss: 0.0195\n",
      "  Epoch 3/5 Batch 600/938 Loss: 0.0084\n",
      "  Epoch 3/5 Batch 700/938 Loss: 0.0317\n",
      "  Epoch 3/5 Batch 800/938 Loss: 0.0283\n",
      "  Epoch 3/5 Batch 900/938 Loss: 0.0024\n",
      "MNIST Pre-train Epoch 3/5 complete.\n",
      "  Epoch 4/5 Batch 0/938 Loss: 0.1457\n",
      "  Epoch 4/5 Batch 100/938 Loss: 0.0008\n",
      "  Epoch 4/5 Batch 200/938 Loss: 0.0354\n",
      "  Epoch 4/5 Batch 300/938 Loss: 0.0145\n",
      "  Epoch 4/5 Batch 400/938 Loss: 0.0059\n",
      "  Epoch 4/5 Batch 500/938 Loss: 0.0248\n",
      "  Epoch 4/5 Batch 600/938 Loss: 0.0704\n",
      "  Epoch 4/5 Batch 700/938 Loss: 0.0564\n",
      "  Epoch 4/5 Batch 800/938 Loss: 0.0016\n",
      "  Epoch 4/5 Batch 900/938 Loss: 0.0118\n",
      "MNIST Pre-train Epoch 4/5 complete.\n",
      "  Epoch 5/5 Batch 0/938 Loss: 0.0247\n",
      "  Epoch 5/5 Batch 100/938 Loss: 0.0008\n",
      "  Epoch 5/5 Batch 200/938 Loss: 0.1172\n",
      "  Epoch 5/5 Batch 300/938 Loss: 0.0285\n",
      "  Epoch 5/5 Batch 400/938 Loss: 0.0015\n",
      "  Epoch 5/5 Batch 500/938 Loss: 0.1741\n",
      "  Epoch 5/5 Batch 600/938 Loss: 0.0023\n",
      "  Epoch 5/5 Batch 700/938 Loss: 0.0210\n",
      "  Epoch 5/5 Batch 800/938 Loss: 0.0651\n",
      "  Epoch 5/5 Batch 900/938 Loss: 0.0062\n",
      "MNIST Pre-train Epoch 5/5 complete.\n",
      "Pre-trained MNIST classifier saved to ./experiments\\20251022-135817_N10\\exp_source_only_N10\\checkpoints\\classifier_mnist_pretrained.pth\n",
      "Scenario: Source Only - Evaluating MNIST pre-trained model directly on SVHN.\n",
      "Source-only classifier model saved to ./experiments\\20251022-135817_N10\\exp_source_only_N10\\checkpoints\\classifier_source_only.pth\n",
      "\n",
      "--- Performing Final Evaluation on SVHN Test Set ---\n",
      "\n",
      "--- Results for Scenario: SOURCE_ONLY (N=10) ---\n",
      "Final Accuracy on SVHN Test Set: 25.90%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2040    0.2001    0.2020      1744\n",
      "           1     0.2546    0.6156    0.3603      5099\n",
      "           2     0.4624    0.2210    0.2991      4149\n",
      "           3     0.2852    0.1378    0.1858      2882\n",
      "           4     0.2476    0.0820    0.1233      2523\n",
      "           5     0.3222    0.2836    0.3017      2384\n",
      "           6     0.3376    0.0804    0.1299      1977\n",
      "           7     0.2974    0.1818    0.2256      2019\n",
      "           8     0.3039    0.1078    0.1592      1660\n",
      "           9     0.1038    0.2207    0.1412      1595\n",
      "\n",
      "    accuracy                         0.2590     26032\n",
      "   macro avg     0.2819    0.2131    0.2128     26032\n",
      "weighted avg     0.2968    0.2590    0.2381     26032\n",
      "\n",
      "============================================================\n",
      "Experiment results saved to: ./experiments\\20251022-135817_N10\\exp_source_only_N10\\results\\source_only_N10_results.json\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-135817_N10\\exp_source_only_N10\\tensorboard_logs\n",
      "\n",
      "Running experiment for scenario: fine_tune with N=10\n",
      "Experiment Loop: Calling run_experiment for scenario=fine_tune, n_samples=10 with args.output_dir=./experiments\\20251022-135817_N10\\exp_fine_tune_N10, gan_model_base_dir=./experiments\\20251022-135817_N10\n",
      "run_experiment: args.output_dir=./experiments\\20251022-135817_N10\\exp_fine_tune_N10, gan_model_base_dir=./experiments\\20251022-135817_N10\n",
      "TensorBoard logs will be saved to: ./experiments\\20251022-135817_N10\\exp_fine_tune_N10\\tensorboard_logs\\20251022-140001_fine_tune_N10\n",
      "\n",
      "===== RUNNING SCENARIO: FINE_TUNE with N=10 on cuda:0 =====\n",
      "Pre-training classifier on MNIST for 5 epochs...\n",
      "  Epoch 1/5 Batch 0/938 Loss: 2.2878\n",
      "  Epoch 1/5 Batch 100/938 Loss: 0.1002\n",
      "  Epoch 1/5 Batch 200/938 Loss: 0.1397\n",
      "  Epoch 1/5 Batch 300/938 Loss: 0.0639\n",
      "  Epoch 1/5 Batch 400/938 Loss: 0.1049\n",
      "  Epoch 1/5 Batch 500/938 Loss: 0.0578\n",
      "  Epoch 1/5 Batch 600/938 Loss: 0.0178\n",
      "  Epoch 1/5 Batch 700/938 Loss: 0.0368\n",
      "  Epoch 1/5 Batch 800/938 Loss: 0.1226\n",
      "  Epoch 1/5 Batch 900/938 Loss: 0.1130\n",
      "MNIST Pre-train Epoch 1/5 complete.\n",
      "  Epoch 2/5 Batch 0/938 Loss: 0.0365\n",
      "  Epoch 2/5 Batch 100/938 Loss: 0.0775\n",
      "  Epoch 2/5 Batch 200/938 Loss: 0.0046\n",
      "  Epoch 2/5 Batch 300/938 Loss: 0.0042\n",
      "  Epoch 2/5 Batch 400/938 Loss: 0.0292\n",
      "  Epoch 2/5 Batch 500/938 Loss: 0.0845\n",
      "  Epoch 2/5 Batch 600/938 Loss: 0.0934\n",
      "  Epoch 2/5 Batch 700/938 Loss: 0.0027\n",
      "  Epoch 2/5 Batch 800/938 Loss: 0.0288\n",
      "  Epoch 2/5 Batch 900/938 Loss: 0.0062\n",
      "MNIST Pre-train Epoch 2/5 complete.\n",
      "  Epoch 3/5 Batch 0/938 Loss: 0.0028\n",
      "  Epoch 3/5 Batch 100/938 Loss: 0.0371\n",
      "  Epoch 3/5 Batch 200/938 Loss: 0.0074\n",
      "  Epoch 3/5 Batch 300/938 Loss: 0.0253\n",
      "  Epoch 3/5 Batch 400/938 Loss: 0.0047\n",
      "  Epoch 3/5 Batch 500/938 Loss: 0.0072\n",
      "  Epoch 3/5 Batch 600/938 Loss: 0.0693\n",
      "  Epoch 3/5 Batch 700/938 Loss: 0.0024\n",
      "  Epoch 3/5 Batch 800/938 Loss: 0.0756\n",
      "  Epoch 3/5 Batch 900/938 Loss: 0.0032\n",
      "MNIST Pre-train Epoch 3/5 complete.\n",
      "  Epoch 4/5 Batch 0/938 Loss: 0.0550\n",
      "  Epoch 4/5 Batch 100/938 Loss: 0.0304\n",
      "  Epoch 4/5 Batch 200/938 Loss: 0.0031\n",
      "  Epoch 4/5 Batch 300/938 Loss: 0.0190\n",
      "  Epoch 4/5 Batch 400/938 Loss: 0.0058\n",
      "  Epoch 4/5 Batch 500/938 Loss: 0.0097\n",
      "  Epoch 4/5 Batch 600/938 Loss: 0.0605\n",
      "  Epoch 4/5 Batch 700/938 Loss: 0.0317\n",
      "  Epoch 4/5 Batch 800/938 Loss: 0.0298\n",
      "  Epoch 4/5 Batch 900/938 Loss: 0.0755\n",
      "MNIST Pre-train Epoch 4/5 complete.\n",
      "  Epoch 5/5 Batch 0/938 Loss: 0.0007\n",
      "  Epoch 5/5 Batch 100/938 Loss: 0.0023\n",
      "  Epoch 5/5 Batch 200/938 Loss: 0.0295\n",
      "  Epoch 5/5 Batch 300/938 Loss: 0.0010\n",
      "  Epoch 5/5 Batch 400/938 Loss: 0.0108\n",
      "  Epoch 5/5 Batch 500/938 Loss: 0.0121\n",
      "  Epoch 5/5 Batch 600/938 Loss: 0.0309\n",
      "  Epoch 5/5 Batch 700/938 Loss: 0.0056\n",
      "  Epoch 5/5 Batch 800/938 Loss: 0.0398\n",
      "  Epoch 5/5 Batch 900/938 Loss: 0.0099\n",
      "MNIST Pre-train Epoch 5/5 complete.\n",
      "Pre-trained MNIST classifier saved to ./experiments\\20251022-135817_N10\\exp_fine_tune_N10\\checkpoints\\classifier_mnist_pretrained.pth\n",
      "Scenario: Fine-tuning on low-resource SVHN data for 20 epochs...\n",
      "  Epoch 1/20 Batch 0/1 Loss: 2.2260\n",
      "SVHN Fine-tune Epoch 1/20 complete.\n",
      "  Epoch 2/20 Batch 0/1 Loss: 1.4351\n",
      "SVHN Fine-tune Epoch 2/20 complete.\n",
      "  Epoch 3/20 Batch 0/1 Loss: 0.9879\n",
      "SVHN Fine-tune Epoch 3/20 complete.\n",
      "  Epoch 4/20 Batch 0/1 Loss: 0.6562\n",
      "SVHN Fine-tune Epoch 4/20 complete.\n",
      "  Epoch 5/20 Batch 0/1 Loss: 0.3505\n",
      "SVHN Fine-tune Epoch 5/20 complete.\n",
      "  Epoch 6/20 Batch 0/1 Loss: 0.1038\n",
      "SVHN Fine-tune Epoch 6/20 complete.\n",
      "  Epoch 7/20 Batch 0/1 Loss: 0.0474\n",
      "SVHN Fine-tune Epoch 7/20 complete.\n",
      "  Epoch 8/20 Batch 0/1 Loss: 0.0059\n",
      "SVHN Fine-tune Epoch 8/20 complete.\n",
      "  Epoch 9/20 Batch 0/1 Loss: 0.0019\n",
      "SVHN Fine-tune Epoch 9/20 complete.\n",
      "  Epoch 10/20 Batch 0/1 Loss: 0.0053\n",
      "SVHN Fine-tune Epoch 10/20 complete.\n",
      "  Epoch 11/20 Batch 0/1 Loss: 0.0004\n",
      "SVHN Fine-tune Epoch 11/20 complete.\n",
      "  Epoch 12/20 Batch 0/1 Loss: 0.0086\n",
      "SVHN Fine-tune Epoch 12/20 complete.\n",
      "  Epoch 13/20 Batch 0/1 Loss: 0.0001\n",
      "SVHN Fine-tune Epoch 13/20 complete.\n",
      "  Epoch 14/20 Batch 0/1 Loss: 0.0247\n",
      "SVHN Fine-tune Epoch 14/20 complete.\n",
      "  Epoch 15/20 Batch 0/1 Loss: 0.0000\n",
      "SVHN Fine-tune Epoch 15/20 complete.\n",
      "  Epoch 16/20 Batch 0/1 Loss: 0.0000\n",
      "SVHN Fine-tune Epoch 16/20 complete.\n",
      "  Epoch 17/20 Batch 0/1 Loss: 0.0001\n",
      "SVHN Fine-tune Epoch 17/20 complete.\n",
      "  Epoch 18/20 Batch 0/1 Loss: 0.0332\n",
      "SVHN Fine-tune Epoch 18/20 complete.\n",
      "  Epoch 19/20 Batch 0/1 Loss: 0.0000\n",
      "SVHN Fine-tune Epoch 19/20 complete.\n",
      "  Epoch 20/20 Batch 0/1 Loss: 0.0002\n",
      "SVHN Fine-tune Epoch 20/20 complete.\n",
      "Fine-tuned classifier model saved to ./experiments\\20251022-135817_N10\\exp_fine_tune_N10\\checkpoints\\classifier_fine_tune.pth\n",
      "\n",
      "--- Performing Final Evaluation on SVHN Test Set ---\n",
      "\n",
      "--- Results for Scenario: FINE_TUNE (N=10) ---\n",
      "Final Accuracy on SVHN Test Set: 18.23%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3047    0.2380    0.2672      1744\n",
      "           1     0.6445    0.0779    0.1389      5099\n",
      "           2     0.5637    0.2090    0.3049      4149\n",
      "           3     0.5595    0.1322    0.2139      2882\n",
      "           4     0.1485    0.1645    0.1561      2523\n",
      "           5     0.1465    0.2513    0.1851      2384\n",
      "           6     0.1064    0.4911    0.1750      1977\n",
      "           7     0.8058    0.0411    0.0782      2019\n",
      "           8     0.1548    0.1416    0.1479      1660\n",
      "           9     0.0910    0.2401    0.1320      1595\n",
      "\n",
      "    accuracy                         0.1823     26032\n",
      "   macro avg     0.3525    0.1987    0.1799     26032\n",
      "weighted avg     0.4123    0.1823    0.1863     26032\n",
      "\n",
      "============================================================\n",
      "Experiment results saved to: ./experiments\\20251022-135817_N10\\exp_fine_tune_N10\\results\\fine_tune_N10_results.json\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-135817_N10\\exp_fine_tune_N10\\tensorboard_logs\n",
      "\n",
      "Running experiment for scenario: traditional_aug with N=10\n",
      "Experiment Loop: Calling run_experiment for scenario=traditional_aug, n_samples=10 with args.output_dir=./experiments\\20251022-135817_N10\\exp_traditional_aug_N10, gan_model_base_dir=./experiments\\20251022-135817_N10\n",
      "run_experiment: args.output_dir=./experiments\\20251022-135817_N10\\exp_traditional_aug_N10, gan_model_base_dir=./experiments\\20251022-135817_N10\n",
      "TensorBoard logs will be saved to: ./experiments\\20251022-135817_N10\\exp_traditional_aug_N10\\tensorboard_logs\\20251022-140126_traditional_aug_N10\n",
      "\n",
      "===== RUNNING SCENARIO: TRADITIONAL_AUG with N=10 on cuda:0 =====\n",
      "Pre-training classifier on MNIST for 5 epochs...\n",
      "  Epoch 1/5 Batch 0/938 Loss: 2.3145\n",
      "  Epoch 1/5 Batch 100/938 Loss: 0.4606\n",
      "  Epoch 1/5 Batch 200/938 Loss: 0.2464\n",
      "  Epoch 1/5 Batch 300/938 Loss: 0.0355\n",
      "  Epoch 1/5 Batch 400/938 Loss: 0.0422\n",
      "  Epoch 1/5 Batch 500/938 Loss: 0.0604\n",
      "  Epoch 1/5 Batch 600/938 Loss: 0.0651\n",
      "  Epoch 1/5 Batch 700/938 Loss: 0.0377\n",
      "  Epoch 1/5 Batch 800/938 Loss: 0.0853\n",
      "  Epoch 1/5 Batch 900/938 Loss: 0.0478\n",
      "MNIST Pre-train Epoch 1/5 complete.\n",
      "  Epoch 2/5 Batch 0/938 Loss: 0.0362\n",
      "  Epoch 2/5 Batch 100/938 Loss: 0.0167\n",
      "  Epoch 2/5 Batch 200/938 Loss: 0.0936\n",
      "  Epoch 2/5 Batch 300/938 Loss: 0.0143\n",
      "  Epoch 2/5 Batch 400/938 Loss: 0.0055\n",
      "  Epoch 2/5 Batch 500/938 Loss: 0.0706\n",
      "  Epoch 2/5 Batch 600/938 Loss: 0.0141\n",
      "  Epoch 2/5 Batch 700/938 Loss: 0.0095\n",
      "  Epoch 2/5 Batch 800/938 Loss: 0.0429\n",
      "  Epoch 2/5 Batch 900/938 Loss: 0.0876\n",
      "MNIST Pre-train Epoch 2/5 complete.\n",
      "  Epoch 3/5 Batch 0/938 Loss: 0.0043\n",
      "  Epoch 3/5 Batch 100/938 Loss: 0.0861\n",
      "  Epoch 3/5 Batch 200/938 Loss: 0.0036\n",
      "  Epoch 3/5 Batch 300/938 Loss: 0.0034\n",
      "  Epoch 3/5 Batch 400/938 Loss: 0.0043\n",
      "  Epoch 3/5 Batch 500/938 Loss: 0.0034\n",
      "  Epoch 3/5 Batch 600/938 Loss: 0.0432\n",
      "  Epoch 3/5 Batch 700/938 Loss: 0.0348\n",
      "  Epoch 3/5 Batch 800/938 Loss: 0.0879\n",
      "  Epoch 3/5 Batch 900/938 Loss: 0.0212\n",
      "MNIST Pre-train Epoch 3/5 complete.\n",
      "  Epoch 4/5 Batch 0/938 Loss: 0.0535\n",
      "  Epoch 4/5 Batch 100/938 Loss: 0.0594\n",
      "  Epoch 4/5 Batch 200/938 Loss: 0.0897\n",
      "  Epoch 4/5 Batch 300/938 Loss: 0.0129\n",
      "  Epoch 4/5 Batch 400/938 Loss: 0.0181\n",
      "  Epoch 4/5 Batch 500/938 Loss: 0.0203\n",
      "  Epoch 4/5 Batch 600/938 Loss: 0.0119\n",
      "  Epoch 4/5 Batch 700/938 Loss: 0.1300\n",
      "  Epoch 4/5 Batch 800/938 Loss: 0.1000\n",
      "  Epoch 4/5 Batch 900/938 Loss: 0.0030\n",
      "MNIST Pre-train Epoch 4/5 complete.\n",
      "  Epoch 5/5 Batch 0/938 Loss: 0.0017\n",
      "  Epoch 5/5 Batch 100/938 Loss: 0.0189\n",
      "  Epoch 5/5 Batch 200/938 Loss: 0.0101\n",
      "  Epoch 5/5 Batch 300/938 Loss: 0.0414\n",
      "  Epoch 5/5 Batch 400/938 Loss: 0.0479\n",
      "  Epoch 5/5 Batch 500/938 Loss: 0.0024\n",
      "  Epoch 5/5 Batch 600/938 Loss: 0.0129\n",
      "  Epoch 5/5 Batch 700/938 Loss: 0.0284\n",
      "  Epoch 5/5 Batch 800/938 Loss: 0.0230\n",
      "  Epoch 5/5 Batch 900/938 Loss: 0.0037\n",
      "MNIST Pre-train Epoch 5/5 complete.\n",
      "Pre-trained MNIST classifier saved to ./experiments\\20251022-135817_N10\\exp_traditional_aug_N10\\checkpoints\\classifier_mnist_pretrained.pth\n",
      "Scenario: Fine-tuning on traditionally augmented low-resource SVHN data for 20 epochs...\n",
      "  Epoch 1/20 Batch 0/1 Loss: 2.7533\n",
      "SVHN Traditional Aug Epoch 1/20 complete.\n",
      "  Epoch 2/20 Batch 0/1 Loss: 2.5076\n",
      "SVHN Traditional Aug Epoch 2/20 complete.\n",
      "  Epoch 3/20 Batch 0/1 Loss: 2.0752\n",
      "SVHN Traditional Aug Epoch 3/20 complete.\n",
      "  Epoch 4/20 Batch 0/1 Loss: 2.3149\n",
      "SVHN Traditional Aug Epoch 4/20 complete.\n",
      "  Epoch 5/20 Batch 0/1 Loss: 1.7511\n",
      "SVHN Traditional Aug Epoch 5/20 complete.\n",
      "  Epoch 6/20 Batch 0/1 Loss: 1.7303\n",
      "SVHN Traditional Aug Epoch 6/20 complete.\n",
      "  Epoch 7/20 Batch 0/1 Loss: 1.4836\n",
      "SVHN Traditional Aug Epoch 7/20 complete.\n",
      "  Epoch 8/20 Batch 0/1 Loss: 1.5985\n",
      "SVHN Traditional Aug Epoch 8/20 complete.\n",
      "  Epoch 9/20 Batch 0/1 Loss: 1.5703\n",
      "SVHN Traditional Aug Epoch 9/20 complete.\n",
      "  Epoch 10/20 Batch 0/1 Loss: 1.2470\n",
      "SVHN Traditional Aug Epoch 10/20 complete.\n",
      "  Epoch 11/20 Batch 0/1 Loss: 1.4796\n",
      "SVHN Traditional Aug Epoch 11/20 complete.\n",
      "  Epoch 12/20 Batch 0/1 Loss: 1.2699\n",
      "SVHN Traditional Aug Epoch 12/20 complete.\n",
      "  Epoch 13/20 Batch 0/1 Loss: 1.3424\n",
      "SVHN Traditional Aug Epoch 13/20 complete.\n",
      "  Epoch 14/20 Batch 0/1 Loss: 0.9078\n",
      "SVHN Traditional Aug Epoch 14/20 complete.\n",
      "  Epoch 15/20 Batch 0/1 Loss: 0.9551\n",
      "SVHN Traditional Aug Epoch 15/20 complete.\n",
      "  Epoch 16/20 Batch 0/1 Loss: 0.7982\n",
      "SVHN Traditional Aug Epoch 16/20 complete.\n",
      "  Epoch 17/20 Batch 0/1 Loss: 0.5158\n",
      "SVHN Traditional Aug Epoch 17/20 complete.\n",
      "  Epoch 18/20 Batch 0/1 Loss: 0.2945\n",
      "SVHN Traditional Aug Epoch 18/20 complete.\n",
      "  Epoch 19/20 Batch 0/1 Loss: 0.5128\n",
      "SVHN Traditional Aug Epoch 19/20 complete.\n",
      "  Epoch 20/20 Batch 0/1 Loss: 0.6093\n",
      "SVHN Traditional Aug Epoch 20/20 complete.\n",
      "Traditional augmented classifier model saved to ./experiments\\20251022-135817_N10\\exp_traditional_aug_N10\\checkpoints\\classifier_traditional_aug.pth\n",
      "\n",
      "--- Performing Final Evaluation on SVHN Test Set ---\n",
      "\n",
      "--- Results for Scenario: TRADITIONAL_AUG (N=10) ---\n",
      "Final Accuracy on SVHN Test Set: 18.75%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1095    0.3211    0.1633      1744\n",
      "           1     0.2664    0.2708    0.2686      5099\n",
      "           2     0.2426    0.2969    0.2670      4149\n",
      "           3     0.3251    0.0871    0.1374      2882\n",
      "           4     0.1350    0.1708    0.1508      2523\n",
      "           5     0.1490    0.2039    0.1722      2384\n",
      "           6     0.1694    0.1254    0.1441      1977\n",
      "           7     0.2934    0.0728    0.1167      2019\n",
      "           8     0.0959    0.0753    0.0844      1660\n",
      "           9     0.1227    0.0125    0.0228      1595\n",
      "\n",
      "    accuracy                         0.1875     26032\n",
      "   macro avg     0.1909    0.1637    0.1527     26032\n",
      "weighted avg     0.2102    0.1875    0.1785     26032\n",
      "\n",
      "============================================================\n",
      "Experiment results saved to: ./experiments\\20251022-135817_N10\\exp_traditional_aug_N10\\results\\traditional_aug_N10_results.json\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-135817_N10\\exp_traditional_aug_N10\\tensorboard_logs\n",
      "\n",
      "Running experiment for scenario: gan_aug with N=10\n",
      "Experiment Loop: Calling run_experiment for scenario=gan_aug, n_samples=10 with args.output_dir=./experiments\\20251022-135817_N10\\exp_gan_aug_N10, gan_model_base_dir=./experiments\\20251022-135817_N10\n",
      "run_experiment: args.output_dir=./experiments\\20251022-135817_N10\\exp_gan_aug_N10, gan_model_base_dir=./experiments\\20251022-135817_N10\n",
      "TensorBoard logs will be saved to: ./experiments\\20251022-135817_N10\\exp_gan_aug_N10\\tensorboard_logs\\20251022-140250_gan_aug_N10\n",
      "\n",
      "===== RUNNING SCENARIO: GAN_AUG with N=10 on cuda:0 =====\n",
      "Pre-training classifier on MNIST for 5 epochs...\n",
      "  Epoch 1/5 Batch 0/938 Loss: 2.3049\n",
      "  Epoch 1/5 Batch 100/938 Loss: 0.1957\n",
      "  Epoch 1/5 Batch 200/938 Loss: 0.1003\n",
      "  Epoch 1/5 Batch 300/938 Loss: 0.0433\n",
      "  Epoch 1/5 Batch 400/938 Loss: 0.1082\n",
      "  Epoch 1/5 Batch 500/938 Loss: 0.1745\n",
      "  Epoch 1/5 Batch 600/938 Loss: 0.0359\n",
      "  Epoch 1/5 Batch 700/938 Loss: 0.0306\n",
      "  Epoch 1/5 Batch 800/938 Loss: 0.1770\n",
      "  Epoch 1/5 Batch 900/938 Loss: 0.0731\n",
      "MNIST Pre-train Epoch 1/5 complete.\n",
      "  Epoch 2/5 Batch 0/938 Loss: 0.0251\n",
      "  Epoch 2/5 Batch 100/938 Loss: 0.0264\n",
      "  Epoch 2/5 Batch 200/938 Loss: 0.0023\n",
      "  Epoch 2/5 Batch 300/938 Loss: 0.0180\n",
      "  Epoch 2/5 Batch 400/938 Loss: 0.0151\n",
      "  Epoch 2/5 Batch 500/938 Loss: 0.0279\n",
      "  Epoch 2/5 Batch 600/938 Loss: 0.0953\n",
      "  Epoch 2/5 Batch 700/938 Loss: 0.0219\n",
      "  Epoch 2/5 Batch 800/938 Loss: 0.1154\n",
      "  Epoch 2/5 Batch 900/938 Loss: 0.0261\n",
      "MNIST Pre-train Epoch 2/5 complete.\n",
      "  Epoch 3/5 Batch 0/938 Loss: 0.0103\n",
      "  Epoch 3/5 Batch 100/938 Loss: 0.0435\n",
      "  Epoch 3/5 Batch 200/938 Loss: 0.0134\n",
      "  Epoch 3/5 Batch 300/938 Loss: 0.0755\n",
      "  Epoch 3/5 Batch 400/938 Loss: 0.0498\n",
      "  Epoch 3/5 Batch 500/938 Loss: 0.0568\n",
      "  Epoch 3/5 Batch 600/938 Loss: 0.0909\n",
      "  Epoch 3/5 Batch 700/938 Loss: 0.0633\n",
      "  Epoch 3/5 Batch 800/938 Loss: 0.0060\n",
      "  Epoch 3/5 Batch 900/938 Loss: 0.1497\n",
      "MNIST Pre-train Epoch 3/5 complete.\n",
      "  Epoch 4/5 Batch 0/938 Loss: 0.0200\n",
      "  Epoch 4/5 Batch 100/938 Loss: 0.0024\n",
      "  Epoch 4/5 Batch 200/938 Loss: 0.0144\n",
      "  Epoch 4/5 Batch 300/938 Loss: 0.0210\n",
      "  Epoch 4/5 Batch 400/938 Loss: 0.0338\n",
      "  Epoch 4/5 Batch 500/938 Loss: 0.0024\n",
      "  Epoch 4/5 Batch 600/938 Loss: 0.0040\n",
      "  Epoch 4/5 Batch 700/938 Loss: 0.0075\n",
      "  Epoch 4/5 Batch 800/938 Loss: 0.1318\n",
      "  Epoch 4/5 Batch 900/938 Loss: 0.0014\n",
      "MNIST Pre-train Epoch 4/5 complete.\n",
      "  Epoch 5/5 Batch 0/938 Loss: 0.0182\n",
      "  Epoch 5/5 Batch 100/938 Loss: 0.0470\n",
      "  Epoch 5/5 Batch 200/938 Loss: 0.0583\n",
      "  Epoch 5/5 Batch 300/938 Loss: 0.0025\n",
      "  Epoch 5/5 Batch 400/938 Loss: 0.0163\n",
      "  Epoch 5/5 Batch 500/938 Loss: 0.0173\n",
      "  Epoch 5/5 Batch 600/938 Loss: 0.0081\n",
      "  Epoch 5/5 Batch 700/938 Loss: 0.0005\n",
      "  Epoch 5/5 Batch 800/938 Loss: 0.0087\n",
      "  Epoch 5/5 Batch 900/938 Loss: 0.0778\n",
      "MNIST Pre-train Epoch 5/5 complete.\n",
      "Pre-trained MNIST classifier saved to ./experiments\\20251022-135817_N10\\exp_gan_aug_N10\\checkpoints\\classifier_mnist_pretrained.pth\n",
      "run_experiment: Calling _run_gan_aug_scenario with output_dir=./experiments\\20251022-135817_N10\\exp_gan_aug_N10, gan_model_base_dir=./experiments\\20251022-135817_N10\n",
      "  _run_gan_aug_scenario: output_dir=./experiments\\20251022-135817_N10\\exp_gan_aug_N10, gan_model_base_dir=./experiments\\20251022-135817_N10\n",
      "Scenario: Fine-tuning on GAN-augmented SVHN data for 20 epochs...\n",
      "  _run_gan_aug_scenario: gan_load_dir=./experiments\\20251022-135817_N10, gan_model_path=./experiments\\20251022-135817_N10\\checkpoints\\gan_generator_n10.pth\n",
      "Generating 5000 synthetic images using GAN...\n",
      "Synthetic images generated.\n",
      "GAN-augmented data loader created.\n",
      "  Epoch 1/20 Batch 0/79 Loss: 2.4758\n",
      "SVHN GAN Aug Epoch 1/20 complete.\n",
      "  Epoch 2/20 Batch 0/79 Loss: 0.0001\n",
      "SVHN GAN Aug Epoch 2/20 complete.\n",
      "  Epoch 3/20 Batch 0/79 Loss: 0.0016\n",
      "SVHN GAN Aug Epoch 3/20 complete.\n",
      "  Epoch 4/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 4/20 complete.\n",
      "  Epoch 5/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 5/20 complete.\n",
      "  Epoch 6/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 6/20 complete.\n",
      "  Epoch 7/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 7/20 complete.\n",
      "  Epoch 8/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 8/20 complete.\n",
      "  Epoch 9/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 9/20 complete.\n",
      "  Epoch 10/20 Batch 0/79 Loss: 0.0001\n",
      "SVHN GAN Aug Epoch 10/20 complete.\n",
      "  Epoch 11/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 11/20 complete.\n",
      "  Epoch 12/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 12/20 complete.\n",
      "  Epoch 13/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 13/20 complete.\n",
      "  Epoch 14/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 14/20 complete.\n",
      "  Epoch 15/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 15/20 complete.\n",
      "  Epoch 16/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 16/20 complete.\n",
      "  Epoch 17/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 17/20 complete.\n",
      "  Epoch 18/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 18/20 complete.\n",
      "  Epoch 19/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 19/20 complete.\n",
      "  Epoch 20/20 Batch 0/79 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 20/20 complete.\n",
      "GAN-augmented classifier model saved to ./experiments\\20251022-135817_N10\\exp_gan_aug_N10\\checkpoints\\classifier_gan_aug.pth\n",
      "\n",
      "--- Performing Final Evaluation on SVHN Test Set ---\n",
      "\n",
      "--- Results for Scenario: GAN_AUG (N=10) ---\n",
      "Final Accuracy on SVHN Test Set: 18.50%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2652    0.1772    0.2124      1744\n",
      "           1     0.3286    0.2218    0.2648      5099\n",
      "           2     0.2977    0.1596    0.2078      4149\n",
      "           3     0.2747    0.0954    0.1416      2882\n",
      "           4     0.1246    0.2442    0.1650      2523\n",
      "           5     0.4035    0.1640    0.2332      2384\n",
      "           6     0.2168    0.0850    0.1221      1977\n",
      "           7     0.1650    0.2922    0.2109      2019\n",
      "           8     0.0839    0.3801    0.1374      1660\n",
      "           9     0.1065    0.0276    0.0438      1595\n",
      "\n",
      "    accuracy                         0.1850     26032\n",
      "   macro avg     0.2267    0.1847    0.1739     26032\n",
      "weighted avg     0.2502    0.1850    0.1893     26032\n",
      "\n",
      "============================================================\n",
      "Experiment results saved to: ./experiments\\20251022-135817_N10\\exp_gan_aug_N10\\results\\gan_aug_N10_results.json\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-135817_N10\\exp_gan_aug_N10\\tensorboard_logs\n",
      "\n",
      "--- Preparing for experiments with N_samples = 100 ---\n",
      "Experiment Loop: base_experiment_run_dir=./experiments\\20251022-140522_N100\n",
      "Experiment Loop: Calling train_gan with output_dir=./experiments\\20251022-140522_N100\n",
      "train_gan: output_dir=./experiments\\20251022-140522_N100\n",
      "Starting GAN training on device: cuda:0\n",
      "TensorBoard logs for GAN training will be saved to: ./experiments\\20251022-140522_N100\\tensorboard_logs\\20251022-140522_GAN_N100\n",
      "Starting GAN training with 100 epochs on 100 SVHN samples...\n",
      "Epoch [1/100] Loss_D: 2.1292 Loss_G: 0.7319\n",
      "Epoch [2/100] Loss_D: 1.6018 Loss_G: 1.1979\n",
      "Epoch [3/100] Loss_D: 1.9201 Loss_G: 1.2571\n",
      "Epoch [4/100] Loss_D: 1.6678 Loss_G: 1.2846\n",
      "Epoch [5/100] Loss_D: 1.4275 Loss_G: 1.3930\n",
      "Epoch [6/100] Loss_D: 1.5725 Loss_G: 1.2303\n",
      "Epoch [7/100] Loss_D: 1.2840 Loss_G: 1.5225\n",
      "Epoch [8/100] Loss_D: 1.3701 Loss_G: 1.6604\n",
      "Epoch [9/100] Loss_D: 0.9532 Loss_G: 1.9105\n",
      "Epoch [10/100] Loss_D: 0.9141 Loss_G: 1.9115\n",
      "Saved generated images for epoch 10.\n",
      "Epoch [11/100] Loss_D: 1.1600 Loss_G: 1.6528\n",
      "Epoch [12/100] Loss_D: 1.5166 Loss_G: 1.4382\n",
      "Epoch [13/100] Loss_D: 1.4024 Loss_G: 1.7316\n",
      "Epoch [14/100] Loss_D: 1.4207 Loss_G: 1.1804\n",
      "Epoch [15/100] Loss_D: 1.1520 Loss_G: 1.4949\n",
      "Epoch [16/100] Loss_D: 1.0481 Loss_G: 1.4946\n",
      "Epoch [17/100] Loss_D: 1.2345 Loss_G: 1.4743\n",
      "Epoch [18/100] Loss_D: 1.3947 Loss_G: 1.6680\n",
      "Epoch [19/100] Loss_D: 1.2667 Loss_G: 1.6616\n",
      "Epoch [20/100] Loss_D: 1.0828 Loss_G: 1.7704\n",
      "Saved generated images for epoch 20.\n",
      "Epoch [21/100] Loss_D: 1.1512 Loss_G: 1.7689\n",
      "Epoch [22/100] Loss_D: 1.5057 Loss_G: 2.1359\n",
      "Epoch [23/100] Loss_D: 1.6585 Loss_G: 2.0887\n",
      "Epoch [24/100] Loss_D: 1.1958 Loss_G: 1.9751\n",
      "Epoch [25/100] Loss_D: 0.8855 Loss_G: 2.5515\n",
      "Epoch [26/100] Loss_D: 0.8763 Loss_G: 2.4123\n",
      "Epoch [27/100] Loss_D: 1.3029 Loss_G: 2.3198\n",
      "Epoch [28/100] Loss_D: 2.0397 Loss_G: 1.7539\n",
      "Epoch [29/100] Loss_D: 1.2264 Loss_G: 2.8634\n",
      "Epoch [30/100] Loss_D: 1.0068 Loss_G: 2.3967\n",
      "Saved generated images for epoch 30.\n",
      "Epoch [31/100] Loss_D: 0.7413 Loss_G: 2.4950\n",
      "Epoch [32/100] Loss_D: 0.9357 Loss_G: 2.4043\n",
      "Epoch [33/100] Loss_D: 0.8509 Loss_G: 2.1883\n",
      "Epoch [34/100] Loss_D: 1.1005 Loss_G: 1.8185\n",
      "Epoch [35/100] Loss_D: 1.0424 Loss_G: 1.9986\n",
      "Epoch [36/100] Loss_D: 0.7330 Loss_G: 2.6704\n",
      "Epoch [37/100] Loss_D: 0.8196 Loss_G: 2.0856\n",
      "Epoch [38/100] Loss_D: 0.9950 Loss_G: 2.1851\n",
      "Epoch [39/100] Loss_D: 1.3654 Loss_G: 1.7972\n",
      "Epoch [40/100] Loss_D: 1.2144 Loss_G: 2.6528\n",
      "Saved generated images for epoch 40.\n",
      "Epoch [41/100] Loss_D: 1.2671 Loss_G: 1.7482\n",
      "Epoch [42/100] Loss_D: 0.9578 Loss_G: 2.3692\n",
      "Epoch [43/100] Loss_D: 0.8824 Loss_G: 2.2465\n",
      "Epoch [44/100] Loss_D: 1.6694 Loss_G: 2.3763\n",
      "Epoch [45/100] Loss_D: 1.2095 Loss_G: 2.2725\n",
      "Epoch [46/100] Loss_D: 0.9928 Loss_G: 1.9797\n",
      "Epoch [47/100] Loss_D: 0.7155 Loss_G: 2.2364\n",
      "Epoch [48/100] Loss_D: 0.7057 Loss_G: 2.2919\n",
      "Epoch [49/100] Loss_D: 0.9708 Loss_G: 2.0921\n",
      "Epoch [50/100] Loss_D: 1.5898 Loss_G: 1.7644\n",
      "Saved generated images for epoch 50.\n",
      "Epoch [51/100] Loss_D: 1.5566 Loss_G: 1.5345\n",
      "Epoch [52/100] Loss_D: 0.8974 Loss_G: 2.1928\n",
      "Epoch [53/100] Loss_D: 0.9107 Loss_G: 2.1927\n",
      "Epoch [54/100] Loss_D: 0.5650 Loss_G: 2.7270\n",
      "Epoch [55/100] Loss_D: 0.8374 Loss_G: 2.3140\n",
      "Epoch [56/100] Loss_D: 1.2490 Loss_G: 2.2145\n",
      "Epoch [57/100] Loss_D: 1.2606 Loss_G: 2.0720\n",
      "Epoch [58/100] Loss_D: 1.0723 Loss_G: 2.5154\n",
      "Epoch [59/100] Loss_D: 1.0823 Loss_G: 2.1279\n",
      "Epoch [60/100] Loss_D: 0.6433 Loss_G: 2.5840\n",
      "Saved generated images for epoch 60.\n",
      "Epoch [61/100] Loss_D: 0.8186 Loss_G: 1.8658\n",
      "Epoch [62/100] Loss_D: 0.8575 Loss_G: 2.2746\n",
      "Epoch [63/100] Loss_D: 0.8535 Loss_G: 2.1564\n",
      "Epoch [64/100] Loss_D: 0.7525 Loss_G: 2.6815\n",
      "Epoch [65/100] Loss_D: 1.1554 Loss_G: 2.5540\n",
      "Epoch [66/100] Loss_D: 0.7904 Loss_G: 2.2561\n",
      "Epoch [67/100] Loss_D: 0.7484 Loss_G: 2.5109\n",
      "Epoch [68/100] Loss_D: 0.6456 Loss_G: 2.5858\n",
      "Epoch [69/100] Loss_D: 0.7552 Loss_G: 2.2430\n",
      "Epoch [70/100] Loss_D: 0.6636 Loss_G: 2.4944\n",
      "Saved generated images for epoch 70.\n",
      "Epoch [71/100] Loss_D: 0.9907 Loss_G: 2.2990\n",
      "Epoch [72/100] Loss_D: 1.1411 Loss_G: 1.6950\n",
      "Epoch [73/100] Loss_D: 0.8467 Loss_G: 2.0439\n",
      "Epoch [74/100] Loss_D: 0.9815 Loss_G: 1.7620\n",
      "Epoch [75/100] Loss_D: 0.7401 Loss_G: 2.2505\n",
      "Epoch [76/100] Loss_D: 0.7919 Loss_G: 2.0410\n",
      "Epoch [77/100] Loss_D: 0.7660 Loss_G: 2.3450\n",
      "Epoch [78/100] Loss_D: 0.7073 Loss_G: 2.1689\n",
      "Epoch [79/100] Loss_D: 0.7177 Loss_G: 1.9981\n",
      "Epoch [80/100] Loss_D: 0.6157 Loss_G: 2.2148\n",
      "Saved generated images for epoch 80.\n",
      "Epoch [81/100] Loss_D: 0.6461 Loss_G: 2.0962\n",
      "Epoch [82/100] Loss_D: 0.7739 Loss_G: 1.8416\n",
      "Epoch [83/100] Loss_D: 0.8225 Loss_G: 2.1112\n",
      "Epoch [84/100] Loss_D: 0.6220 Loss_G: 2.3046\n",
      "Epoch [85/100] Loss_D: 0.6177 Loss_G: 2.1127\n",
      "Epoch [86/100] Loss_D: 0.5857 Loss_G: 2.4097\n",
      "Epoch [87/100] Loss_D: 0.8818 Loss_G: 2.0440\n",
      "Epoch [88/100] Loss_D: 1.0439 Loss_G: 2.3089\n",
      "Epoch [89/100] Loss_D: 1.0768 Loss_G: 1.8424\n",
      "Epoch [90/100] Loss_D: 0.9342 Loss_G: 2.5773\n",
      "Saved generated images for epoch 90.\n",
      "Epoch [91/100] Loss_D: 1.1260 Loss_G: 1.9220\n",
      "Epoch [92/100] Loss_D: 0.9324 Loss_G: 2.4589\n",
      "Epoch [93/100] Loss_D: 0.9059 Loss_G: 2.0733\n",
      "Epoch [94/100] Loss_D: 0.8430 Loss_G: 2.1483\n",
      "Epoch [95/100] Loss_D: 1.1153 Loss_G: 1.7841\n",
      "Epoch [96/100] Loss_D: 0.8737 Loss_G: 2.3136\n",
      "Epoch [97/100] Loss_D: 1.0808 Loss_G: 1.7908\n",
      "Epoch [98/100] Loss_D: 1.1265 Loss_G: 2.1268\n",
      "Epoch [99/100] Loss_D: 0.8427 Loss_G: 2.3969\n",
      "Epoch [100/100] Loss_D: 0.8185 Loss_G: 2.2114\n",
      "Saved generated images for epoch 100.\n",
      "Finished GAN Training. Generator model saved to ./experiments\\20251022-140522_N100\\checkpoints\\gan_generator_n100.pth\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-140522_N100\\tensorboard_logs\n",
      "GAN training for n_samples=100 completed.\n",
      "\n",
      "Running experiment for scenario: source_only with N=100\n",
      "Experiment Loop: Calling run_experiment for scenario=source_only, n_samples=100 with args.output_dir=./experiments\\20251022-140522_N100\\exp_source_only_N100, gan_model_base_dir=./experiments\\20251022-140522_N100\n",
      "run_experiment: args.output_dir=./experiments\\20251022-140522_N100\\exp_source_only_N100, gan_model_base_dir=./experiments\\20251022-140522_N100\n",
      "TensorBoard logs will be saved to: ./experiments\\20251022-140522_N100\\exp_source_only_N100\\tensorboard_logs\\20251022-140535_source_only_N100\n",
      "\n",
      "===== RUNNING SCENARIO: SOURCE_ONLY with N=100 on cuda:0 =====\n",
      "Pre-training classifier on MNIST for 5 epochs...\n",
      "  Epoch 1/5 Batch 0/938 Loss: 2.2939\n",
      "  Epoch 1/5 Batch 100/938 Loss: 0.2177\n",
      "  Epoch 1/5 Batch 200/938 Loss: 0.1670\n",
      "  Epoch 1/5 Batch 300/938 Loss: 0.1022\n",
      "  Epoch 1/5 Batch 400/938 Loss: 0.0175\n",
      "  Epoch 1/5 Batch 500/938 Loss: 0.0153\n",
      "  Epoch 1/5 Batch 600/938 Loss: 0.0384\n",
      "  Epoch 1/5 Batch 700/938 Loss: 0.1126\n",
      "  Epoch 1/5 Batch 800/938 Loss: 0.1121\n",
      "  Epoch 1/5 Batch 900/938 Loss: 0.0574\n",
      "MNIST Pre-train Epoch 1/5 complete.\n",
      "  Epoch 2/5 Batch 0/938 Loss: 0.1855\n",
      "  Epoch 2/5 Batch 100/938 Loss: 0.0330\n",
      "  Epoch 2/5 Batch 200/938 Loss: 0.0417\n",
      "  Epoch 2/5 Batch 300/938 Loss: 0.0766\n",
      "  Epoch 2/5 Batch 400/938 Loss: 0.0155\n",
      "  Epoch 2/5 Batch 500/938 Loss: 0.0025\n",
      "  Epoch 2/5 Batch 600/938 Loss: 0.0121\n",
      "  Epoch 2/5 Batch 700/938 Loss: 0.0336\n",
      "  Epoch 2/5 Batch 800/938 Loss: 0.0036\n",
      "  Epoch 2/5 Batch 900/938 Loss: 0.0012\n",
      "MNIST Pre-train Epoch 2/5 complete.\n",
      "  Epoch 3/5 Batch 0/938 Loss: 0.0172\n",
      "  Epoch 3/5 Batch 100/938 Loss: 0.0028\n",
      "  Epoch 3/5 Batch 200/938 Loss: 0.0546\n",
      "  Epoch 3/5 Batch 300/938 Loss: 0.0084\n",
      "  Epoch 3/5 Batch 400/938 Loss: 0.0066\n",
      "  Epoch 3/5 Batch 500/938 Loss: 0.0287\n",
      "  Epoch 3/5 Batch 600/938 Loss: 0.0339\n",
      "  Epoch 3/5 Batch 700/938 Loss: 0.0007\n",
      "  Epoch 3/5 Batch 800/938 Loss: 0.0009\n",
      "  Epoch 3/5 Batch 900/938 Loss: 0.0583\n",
      "MNIST Pre-train Epoch 3/5 complete.\n",
      "  Epoch 4/5 Batch 0/938 Loss: 0.0690\n",
      "  Epoch 4/5 Batch 100/938 Loss: 0.0016\n",
      "  Epoch 4/5 Batch 200/938 Loss: 0.0025\n",
      "  Epoch 4/5 Batch 300/938 Loss: 0.0223\n",
      "  Epoch 4/5 Batch 400/938 Loss: 0.0033\n",
      "  Epoch 4/5 Batch 500/938 Loss: 0.0058\n",
      "  Epoch 4/5 Batch 600/938 Loss: 0.1390\n",
      "  Epoch 4/5 Batch 700/938 Loss: 0.0031\n",
      "  Epoch 4/5 Batch 800/938 Loss: 0.0097\n",
      "  Epoch 4/5 Batch 900/938 Loss: 0.0152\n",
      "MNIST Pre-train Epoch 4/5 complete.\n",
      "  Epoch 5/5 Batch 0/938 Loss: 0.0075\n",
      "  Epoch 5/5 Batch 100/938 Loss: 0.0262\n",
      "  Epoch 5/5 Batch 200/938 Loss: 0.0003\n",
      "  Epoch 5/5 Batch 300/938 Loss: 0.0008\n",
      "  Epoch 5/5 Batch 400/938 Loss: 0.0027\n",
      "  Epoch 5/5 Batch 500/938 Loss: 0.0641\n",
      "  Epoch 5/5 Batch 600/938 Loss: 0.0001\n",
      "  Epoch 5/5 Batch 700/938 Loss: 0.0051\n",
      "  Epoch 5/5 Batch 800/938 Loss: 0.0096\n",
      "  Epoch 5/5 Batch 900/938 Loss: 0.0704\n",
      "MNIST Pre-train Epoch 5/5 complete.\n",
      "Pre-trained MNIST classifier saved to ./experiments\\20251022-140522_N100\\exp_source_only_N100\\checkpoints\\classifier_mnist_pretrained.pth\n",
      "Scenario: Source Only - Evaluating MNIST pre-trained model directly on SVHN.\n",
      "Source-only classifier model saved to ./experiments\\20251022-140522_N100\\exp_source_only_N100\\checkpoints\\classifier_source_only.pth\n",
      "\n",
      "--- Performing Final Evaluation on SVHN Test Set ---\n",
      "\n",
      "--- Results for Scenario: SOURCE_ONLY (N=100) ---\n",
      "Final Accuracy on SVHN Test Set: 27.11%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1793    0.1583    0.1681      1744\n",
      "           1     0.3588    0.3652    0.3619      5099\n",
      "           2     0.5672    0.2145    0.3113      4149\n",
      "           3     0.7090    0.1124    0.1941      2882\n",
      "           4     0.1545    0.4253    0.2266      2523\n",
      "           5     0.4801    0.2580    0.3356      2384\n",
      "           6     0.2248    0.1831    0.2018      1977\n",
      "           7     0.2080    0.5077    0.2951      2019\n",
      "           8     0.2364    0.2139    0.2245      1660\n",
      "           9     0.2716    0.1718    0.2104      1595\n",
      "\n",
      "    accuracy                         0.2711     26032\n",
      "   macro avg     0.3390    0.2610    0.2530     26032\n",
      "weighted avg     0.3750    0.2711    0.2714     26032\n",
      "\n",
      "============================================================\n",
      "Experiment results saved to: ./experiments\\20251022-140522_N100\\exp_source_only_N100\\results\\source_only_N100_results.json\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-140522_N100\\exp_source_only_N100\\tensorboard_logs\n",
      "\n",
      "Running experiment for scenario: fine_tune with N=100\n",
      "Experiment Loop: Calling run_experiment for scenario=fine_tune, n_samples=100 with args.output_dir=./experiments\\20251022-140522_N100\\exp_fine_tune_N100, gan_model_base_dir=./experiments\\20251022-140522_N100\n",
      "run_experiment: args.output_dir=./experiments\\20251022-140522_N100\\exp_fine_tune_N100, gan_model_base_dir=./experiments\\20251022-140522_N100\n",
      "TensorBoard logs will be saved to: ./experiments\\20251022-140522_N100\\exp_fine_tune_N100\\tensorboard_logs\\20251022-140656_fine_tune_N100\n",
      "\n",
      "===== RUNNING SCENARIO: FINE_TUNE with N=100 on cuda:0 =====\n",
      "Pre-training classifier on MNIST for 5 epochs...\n",
      "  Epoch 1/5 Batch 0/938 Loss: 2.3097\n",
      "  Epoch 1/5 Batch 100/938 Loss: 0.0806\n",
      "  Epoch 1/5 Batch 200/938 Loss: 0.2658\n",
      "  Epoch 1/5 Batch 300/938 Loss: 0.1543\n",
      "  Epoch 1/5 Batch 400/938 Loss: 0.1046\n",
      "  Epoch 1/5 Batch 500/938 Loss: 0.0631\n",
      "  Epoch 1/5 Batch 600/938 Loss: 0.0545\n",
      "  Epoch 1/5 Batch 700/938 Loss: 0.2498\n",
      "  Epoch 1/5 Batch 800/938 Loss: 0.0532\n",
      "  Epoch 1/5 Batch 900/938 Loss: 0.0294\n",
      "MNIST Pre-train Epoch 1/5 complete.\n",
      "  Epoch 2/5 Batch 0/938 Loss: 0.0070\n",
      "  Epoch 2/5 Batch 100/938 Loss: 0.0433\n",
      "  Epoch 2/5 Batch 200/938 Loss: 0.0693\n",
      "  Epoch 2/5 Batch 300/938 Loss: 0.0109\n",
      "  Epoch 2/5 Batch 400/938 Loss: 0.0154\n",
      "  Epoch 2/5 Batch 500/938 Loss: 0.0372\n",
      "  Epoch 2/5 Batch 600/938 Loss: 0.0035\n",
      "  Epoch 2/5 Batch 700/938 Loss: 0.1373\n",
      "  Epoch 2/5 Batch 800/938 Loss: 0.0273\n",
      "  Epoch 2/5 Batch 900/938 Loss: 0.0107\n",
      "MNIST Pre-train Epoch 2/5 complete.\n",
      "  Epoch 3/5 Batch 0/938 Loss: 0.0203\n",
      "  Epoch 3/5 Batch 100/938 Loss: 0.0258\n",
      "  Epoch 3/5 Batch 200/938 Loss: 0.0375\n",
      "  Epoch 3/5 Batch 300/938 Loss: 0.0698\n",
      "  Epoch 3/5 Batch 400/938 Loss: 0.0193\n",
      "  Epoch 3/5 Batch 500/938 Loss: 0.0177\n",
      "  Epoch 3/5 Batch 600/938 Loss: 0.0208\n",
      "  Epoch 3/5 Batch 700/938 Loss: 0.0295\n",
      "  Epoch 3/5 Batch 800/938 Loss: 0.0235\n",
      "  Epoch 3/5 Batch 900/938 Loss: 0.0017\n",
      "MNIST Pre-train Epoch 3/5 complete.\n",
      "  Epoch 4/5 Batch 0/938 Loss: 0.0225\n",
      "  Epoch 4/5 Batch 100/938 Loss: 0.0052\n",
      "  Epoch 4/5 Batch 200/938 Loss: 0.0594\n",
      "  Epoch 4/5 Batch 300/938 Loss: 0.0020\n",
      "  Epoch 4/5 Batch 400/938 Loss: 0.0381\n",
      "  Epoch 4/5 Batch 500/938 Loss: 0.0040\n",
      "  Epoch 4/5 Batch 600/938 Loss: 0.0008\n",
      "  Epoch 4/5 Batch 700/938 Loss: 0.0010\n",
      "  Epoch 4/5 Batch 800/938 Loss: 0.0683\n",
      "  Epoch 4/5 Batch 900/938 Loss: 0.0430\n",
      "MNIST Pre-train Epoch 4/5 complete.\n",
      "  Epoch 5/5 Batch 0/938 Loss: 0.0090\n",
      "  Epoch 5/5 Batch 100/938 Loss: 0.0414\n",
      "  Epoch 5/5 Batch 200/938 Loss: 0.0068\n",
      "  Epoch 5/5 Batch 300/938 Loss: 0.0148\n",
      "  Epoch 5/5 Batch 400/938 Loss: 0.0367\n",
      "  Epoch 5/5 Batch 500/938 Loss: 0.0015\n",
      "  Epoch 5/5 Batch 600/938 Loss: 0.1160\n",
      "  Epoch 5/5 Batch 700/938 Loss: 0.0004\n",
      "  Epoch 5/5 Batch 800/938 Loss: 0.0400\n",
      "  Epoch 5/5 Batch 900/938 Loss: 0.0252\n",
      "MNIST Pre-train Epoch 5/5 complete.\n",
      "Pre-trained MNIST classifier saved to ./experiments\\20251022-140522_N100\\exp_fine_tune_N100\\checkpoints\\classifier_mnist_pretrained.pth\n",
      "Scenario: Fine-tuning on low-resource SVHN data for 20 epochs...\n",
      "  Epoch 1/20 Batch 0/2 Loss: 2.7065\n",
      "SVHN Fine-tune Epoch 1/20 complete.\n",
      "  Epoch 2/20 Batch 0/2 Loss: 2.0242\n",
      "SVHN Fine-tune Epoch 2/20 complete.\n",
      "  Epoch 3/20 Batch 0/2 Loss: 1.8750\n",
      "SVHN Fine-tune Epoch 3/20 complete.\n",
      "  Epoch 4/20 Batch 0/2 Loss: 1.6215\n",
      "SVHN Fine-tune Epoch 4/20 complete.\n",
      "  Epoch 5/20 Batch 0/2 Loss: 1.3651\n",
      "SVHN Fine-tune Epoch 5/20 complete.\n",
      "  Epoch 6/20 Batch 0/2 Loss: 0.9718\n",
      "SVHN Fine-tune Epoch 6/20 complete.\n",
      "  Epoch 7/20 Batch 0/2 Loss: 0.9069\n",
      "SVHN Fine-tune Epoch 7/20 complete.\n",
      "  Epoch 8/20 Batch 0/2 Loss: 0.9301\n",
      "SVHN Fine-tune Epoch 8/20 complete.\n",
      "  Epoch 9/20 Batch 0/2 Loss: 0.5446\n",
      "SVHN Fine-tune Epoch 9/20 complete.\n",
      "  Epoch 10/20 Batch 0/2 Loss: 0.4044\n",
      "SVHN Fine-tune Epoch 10/20 complete.\n",
      "  Epoch 11/20 Batch 0/2 Loss: 0.3855\n",
      "SVHN Fine-tune Epoch 11/20 complete.\n",
      "  Epoch 12/20 Batch 0/2 Loss: 0.1407\n",
      "SVHN Fine-tune Epoch 12/20 complete.\n",
      "  Epoch 13/20 Batch 0/2 Loss: 0.1070\n",
      "SVHN Fine-tune Epoch 13/20 complete.\n",
      "  Epoch 14/20 Batch 0/2 Loss: 0.1755\n",
      "SVHN Fine-tune Epoch 14/20 complete.\n",
      "  Epoch 15/20 Batch 0/2 Loss: 0.0652\n",
      "SVHN Fine-tune Epoch 15/20 complete.\n",
      "  Epoch 16/20 Batch 0/2 Loss: 0.0385\n",
      "SVHN Fine-tune Epoch 16/20 complete.\n",
      "  Epoch 17/20 Batch 0/2 Loss: 0.0097\n",
      "SVHN Fine-tune Epoch 17/20 complete.\n",
      "  Epoch 18/20 Batch 0/2 Loss: 0.0113\n",
      "SVHN Fine-tune Epoch 18/20 complete.\n",
      "  Epoch 19/20 Batch 0/2 Loss: 0.0070\n",
      "SVHN Fine-tune Epoch 19/20 complete.\n",
      "  Epoch 20/20 Batch 0/2 Loss: 0.0098\n",
      "SVHN Fine-tune Epoch 20/20 complete.\n",
      "Fine-tuned classifier model saved to ./experiments\\20251022-140522_N100\\exp_fine_tune_N100\\checkpoints\\classifier_fine_tune.pth\n",
      "\n",
      "--- Performing Final Evaluation on SVHN Test Set ---\n",
      "\n",
      "--- Results for Scenario: FINE_TUNE (N=100) ---\n",
      "Final Accuracy on SVHN Test Set: 37.89%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3350    0.3016    0.3174      1744\n",
      "           1     0.7754    0.4495    0.5691      5099\n",
      "           2     0.5739    0.3921    0.4659      4149\n",
      "           3     0.3154    0.3904    0.3489      2882\n",
      "           4     0.4173    0.2422    0.3065      2523\n",
      "           5     0.4473    0.3414    0.3873      2384\n",
      "           6     0.2132    0.3232    0.2569      1977\n",
      "           7     0.3710    0.5513    0.4435      2019\n",
      "           8     0.1881    0.3169    0.2360      1660\n",
      "           9     0.1950    0.3699    0.2554      1595\n",
      "\n",
      "    accuracy                         0.3789     26032\n",
      "   macro avg     0.3832    0.3678    0.3587     26032\n",
      "weighted avg     0.4510    0.3789    0.3954     26032\n",
      "\n",
      "============================================================\n",
      "Experiment results saved to: ./experiments\\20251022-140522_N100\\exp_fine_tune_N100\\results\\fine_tune_N100_results.json\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-140522_N100\\exp_fine_tune_N100\\tensorboard_logs\n",
      "\n",
      "Running experiment for scenario: traditional_aug with N=100\n",
      "Experiment Loop: Calling run_experiment for scenario=traditional_aug, n_samples=100 with args.output_dir=./experiments\\20251022-140522_N100\\exp_traditional_aug_N100, gan_model_base_dir=./experiments\\20251022-140522_N100\n",
      "run_experiment: args.output_dir=./experiments\\20251022-140522_N100\\exp_traditional_aug_N100, gan_model_base_dir=./experiments\\20251022-140522_N100\n",
      "TensorBoard logs will be saved to: ./experiments\\20251022-140522_N100\\exp_traditional_aug_N100\\tensorboard_logs\\20251022-140826_traditional_aug_N100\n",
      "\n",
      "===== RUNNING SCENARIO: TRADITIONAL_AUG with N=100 on cuda:0 =====\n",
      "Pre-training classifier on MNIST for 5 epochs...\n",
      "  Epoch 1/5 Batch 0/938 Loss: 2.3142\n",
      "  Epoch 1/5 Batch 100/938 Loss: 0.2736\n",
      "  Epoch 1/5 Batch 200/938 Loss: 0.0942\n",
      "  Epoch 1/5 Batch 300/938 Loss: 0.0783\n",
      "  Epoch 1/5 Batch 400/938 Loss: 0.1555\n",
      "  Epoch 1/5 Batch 500/938 Loss: 0.1266\n",
      "  Epoch 1/5 Batch 600/938 Loss: 0.0474\n",
      "  Epoch 1/5 Batch 700/938 Loss: 0.0059\n",
      "  Epoch 1/5 Batch 800/938 Loss: 0.0725\n",
      "  Epoch 1/5 Batch 900/938 Loss: 0.0184\n",
      "MNIST Pre-train Epoch 1/5 complete.\n",
      "  Epoch 2/5 Batch 0/938 Loss: 0.0388\n",
      "  Epoch 2/5 Batch 100/938 Loss: 0.0074\n",
      "  Epoch 2/5 Batch 200/938 Loss: 0.0027\n",
      "  Epoch 2/5 Batch 300/938 Loss: 0.0069\n",
      "  Epoch 2/5 Batch 400/938 Loss: 0.0017\n",
      "  Epoch 2/5 Batch 500/938 Loss: 0.0030\n",
      "  Epoch 2/5 Batch 600/938 Loss: 0.1457\n",
      "  Epoch 2/5 Batch 700/938 Loss: 0.1149\n",
      "  Epoch 2/5 Batch 800/938 Loss: 0.0630\n",
      "  Epoch 2/5 Batch 900/938 Loss: 0.0300\n",
      "MNIST Pre-train Epoch 2/5 complete.\n",
      "  Epoch 3/5 Batch 0/938 Loss: 0.0019\n",
      "  Epoch 3/5 Batch 100/938 Loss: 0.0548\n",
      "  Epoch 3/5 Batch 200/938 Loss: 0.0205\n",
      "  Epoch 3/5 Batch 300/938 Loss: 0.0092\n",
      "  Epoch 3/5 Batch 400/938 Loss: 0.0437\n",
      "  Epoch 3/5 Batch 500/938 Loss: 0.0321\n",
      "  Epoch 3/5 Batch 600/938 Loss: 0.0335\n",
      "  Epoch 3/5 Batch 700/938 Loss: 0.0077\n",
      "  Epoch 3/5 Batch 800/938 Loss: 0.0269\n",
      "  Epoch 3/5 Batch 900/938 Loss: 0.0078\n",
      "MNIST Pre-train Epoch 3/5 complete.\n",
      "  Epoch 4/5 Batch 0/938 Loss: 0.0010\n",
      "  Epoch 4/5 Batch 100/938 Loss: 0.0449\n",
      "  Epoch 4/5 Batch 200/938 Loss: 0.0007\n",
      "  Epoch 4/5 Batch 300/938 Loss: 0.0773\n",
      "  Epoch 4/5 Batch 400/938 Loss: 0.0099\n",
      "  Epoch 4/5 Batch 500/938 Loss: 0.0194\n",
      "  Epoch 4/5 Batch 600/938 Loss: 0.0095\n",
      "  Epoch 4/5 Batch 700/938 Loss: 0.0019\n",
      "  Epoch 4/5 Batch 800/938 Loss: 0.0719\n",
      "  Epoch 4/5 Batch 900/938 Loss: 0.0012\n",
      "MNIST Pre-train Epoch 4/5 complete.\n",
      "  Epoch 5/5 Batch 0/938 Loss: 0.0137\n",
      "  Epoch 5/5 Batch 100/938 Loss: 0.0004\n",
      "  Epoch 5/5 Batch 200/938 Loss: 0.0107\n",
      "  Epoch 5/5 Batch 300/938 Loss: 0.0297\n",
      "  Epoch 5/5 Batch 400/938 Loss: 0.0442\n",
      "  Epoch 5/5 Batch 500/938 Loss: 0.1252\n",
      "  Epoch 5/5 Batch 600/938 Loss: 0.0006\n",
      "  Epoch 5/5 Batch 700/938 Loss: 0.0061\n",
      "  Epoch 5/5 Batch 800/938 Loss: 0.1139\n",
      "  Epoch 5/5 Batch 900/938 Loss: 0.1245\n",
      "MNIST Pre-train Epoch 5/5 complete.\n",
      "Pre-trained MNIST classifier saved to ./experiments\\20251022-140522_N100\\exp_traditional_aug_N100\\checkpoints\\classifier_mnist_pretrained.pth\n",
      "Scenario: Fine-tuning on traditionally augmented low-resource SVHN data for 20 epochs...\n",
      "  Epoch 1/20 Batch 0/2 Loss: 3.6023\n",
      "SVHN Traditional Aug Epoch 1/20 complete.\n",
      "  Epoch 2/20 Batch 0/2 Loss: 2.3765\n",
      "SVHN Traditional Aug Epoch 2/20 complete.\n",
      "  Epoch 3/20 Batch 0/2 Loss: 2.1549\n",
      "SVHN Traditional Aug Epoch 3/20 complete.\n",
      "  Epoch 4/20 Batch 0/2 Loss: 2.1045\n",
      "SVHN Traditional Aug Epoch 4/20 complete.\n",
      "  Epoch 5/20 Batch 0/2 Loss: 2.0241\n",
      "SVHN Traditional Aug Epoch 5/20 complete.\n",
      "  Epoch 6/20 Batch 0/2 Loss: 2.0049\n",
      "SVHN Traditional Aug Epoch 6/20 complete.\n",
      "  Epoch 7/20 Batch 0/2 Loss: 1.8500\n",
      "SVHN Traditional Aug Epoch 7/20 complete.\n",
      "  Epoch 8/20 Batch 0/2 Loss: 1.7734\n",
      "SVHN Traditional Aug Epoch 8/20 complete.\n",
      "  Epoch 9/20 Batch 0/2 Loss: 1.8401\n",
      "SVHN Traditional Aug Epoch 9/20 complete.\n",
      "  Epoch 10/20 Batch 0/2 Loss: 1.5701\n",
      "SVHN Traditional Aug Epoch 10/20 complete.\n",
      "  Epoch 11/20 Batch 0/2 Loss: 1.5624\n",
      "SVHN Traditional Aug Epoch 11/20 complete.\n",
      "  Epoch 12/20 Batch 0/2 Loss: 1.4524\n",
      "SVHN Traditional Aug Epoch 12/20 complete.\n",
      "  Epoch 13/20 Batch 0/2 Loss: 1.1333\n",
      "SVHN Traditional Aug Epoch 13/20 complete.\n",
      "  Epoch 14/20 Batch 0/2 Loss: 1.4036\n",
      "SVHN Traditional Aug Epoch 14/20 complete.\n",
      "  Epoch 15/20 Batch 0/2 Loss: 1.1557\n",
      "SVHN Traditional Aug Epoch 15/20 complete.\n",
      "  Epoch 16/20 Batch 0/2 Loss: 1.0059\n",
      "SVHN Traditional Aug Epoch 16/20 complete.\n",
      "  Epoch 17/20 Batch 0/2 Loss: 1.0537\n",
      "SVHN Traditional Aug Epoch 17/20 complete.\n",
      "  Epoch 18/20 Batch 0/2 Loss: 0.9778\n",
      "SVHN Traditional Aug Epoch 18/20 complete.\n",
      "  Epoch 19/20 Batch 0/2 Loss: 1.0319\n",
      "SVHN Traditional Aug Epoch 19/20 complete.\n",
      "  Epoch 20/20 Batch 0/2 Loss: 0.7056\n",
      "SVHN Traditional Aug Epoch 20/20 complete.\n",
      "Traditional augmented classifier model saved to ./experiments\\20251022-140522_N100\\exp_traditional_aug_N100\\checkpoints\\classifier_traditional_aug.pth\n",
      "\n",
      "--- Performing Final Evaluation on SVHN Test Set ---\n",
      "\n",
      "--- Results for Scenario: TRADITIONAL_AUG (N=100) ---\n",
      "Final Accuracy on SVHN Test Set: 39.12%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3420    0.1651    0.2227      1744\n",
      "           1     0.5469    0.5654    0.5560      5099\n",
      "           2     0.6138    0.3692    0.4611      4149\n",
      "           3     0.4359    0.2609    0.3265      2882\n",
      "           4     0.3728    0.3167    0.3425      2523\n",
      "           5     0.3608    0.4018    0.3802      2384\n",
      "           6     0.2460    0.4107    0.3077      1977\n",
      "           7     0.3312    0.5468    0.4126      2019\n",
      "           8     0.2353    0.3867    0.2926      1660\n",
      "           9     0.2687    0.2589    0.2637      1595\n",
      "\n",
      "    accuracy                         0.3912     26032\n",
      "   macro avg     0.3754    0.3682    0.3566     26032\n",
      "weighted avg     0.4211    0.3912    0.3917     26032\n",
      "\n",
      "============================================================\n",
      "Experiment results saved to: ./experiments\\20251022-140522_N100\\exp_traditional_aug_N100\\results\\traditional_aug_N100_results.json\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-140522_N100\\exp_traditional_aug_N100\\tensorboard_logs\n",
      "\n",
      "Running experiment for scenario: gan_aug with N=100\n",
      "Experiment Loop: Calling run_experiment for scenario=gan_aug, n_samples=100 with args.output_dir=./experiments\\20251022-140522_N100\\exp_gan_aug_N100, gan_model_base_dir=./experiments\\20251022-140522_N100\n",
      "run_experiment: args.output_dir=./experiments\\20251022-140522_N100\\exp_gan_aug_N100, gan_model_base_dir=./experiments\\20251022-140522_N100\n",
      "TensorBoard logs will be saved to: ./experiments\\20251022-140522_N100\\exp_gan_aug_N100\\tensorboard_logs\\20251022-140952_gan_aug_N100\n",
      "\n",
      "===== RUNNING SCENARIO: GAN_AUG with N=100 on cuda:0 =====\n",
      "Pre-training classifier on MNIST for 5 epochs...\n",
      "  Epoch 1/5 Batch 0/938 Loss: 2.3200\n",
      "  Epoch 1/5 Batch 100/938 Loss: 0.3205\n",
      "  Epoch 1/5 Batch 200/938 Loss: 0.0813\n",
      "  Epoch 1/5 Batch 300/938 Loss: 0.0509\n",
      "  Epoch 1/5 Batch 400/938 Loss: 0.0255\n",
      "  Epoch 1/5 Batch 500/938 Loss: 0.0381\n",
      "  Epoch 1/5 Batch 600/938 Loss: 0.0198\n",
      "  Epoch 1/5 Batch 700/938 Loss: 0.1807\n",
      "  Epoch 1/5 Batch 800/938 Loss: 0.2331\n",
      "  Epoch 1/5 Batch 900/938 Loss: 0.0078\n",
      "MNIST Pre-train Epoch 1/5 complete.\n",
      "  Epoch 2/5 Batch 0/938 Loss: 0.0086\n",
      "  Epoch 2/5 Batch 100/938 Loss: 0.0077\n",
      "  Epoch 2/5 Batch 200/938 Loss: 0.0045\n",
      "  Epoch 2/5 Batch 300/938 Loss: 0.1190\n",
      "  Epoch 2/5 Batch 400/938 Loss: 0.1451\n",
      "  Epoch 2/5 Batch 500/938 Loss: 0.0051\n",
      "  Epoch 2/5 Batch 600/938 Loss: 0.1245\n",
      "  Epoch 2/5 Batch 700/938 Loss: 0.0120\n",
      "  Epoch 2/5 Batch 800/938 Loss: 0.0518\n",
      "  Epoch 2/5 Batch 900/938 Loss: 0.0309\n",
      "MNIST Pre-train Epoch 2/5 complete.\n",
      "  Epoch 3/5 Batch 0/938 Loss: 0.0095\n",
      "  Epoch 3/5 Batch 100/938 Loss: 0.0303\n",
      "  Epoch 3/5 Batch 200/938 Loss: 0.0146\n",
      "  Epoch 3/5 Batch 300/938 Loss: 0.0343\n",
      "  Epoch 3/5 Batch 400/938 Loss: 0.0146\n",
      "  Epoch 3/5 Batch 500/938 Loss: 0.0122\n",
      "  Epoch 3/5 Batch 600/938 Loss: 0.0130\n",
      "  Epoch 3/5 Batch 700/938 Loss: 0.0437\n",
      "  Epoch 3/5 Batch 800/938 Loss: 0.0010\n",
      "  Epoch 3/5 Batch 900/938 Loss: 0.0205\n",
      "MNIST Pre-train Epoch 3/5 complete.\n",
      "  Epoch 4/5 Batch 0/938 Loss: 0.0233\n",
      "  Epoch 4/5 Batch 100/938 Loss: 0.0071\n",
      "  Epoch 4/5 Batch 200/938 Loss: 0.0741\n",
      "  Epoch 4/5 Batch 300/938 Loss: 0.0422\n",
      "  Epoch 4/5 Batch 400/938 Loss: 0.0045\n",
      "  Epoch 4/5 Batch 500/938 Loss: 0.1137\n",
      "  Epoch 4/5 Batch 600/938 Loss: 0.0040\n",
      "  Epoch 4/5 Batch 700/938 Loss: 0.0114\n",
      "  Epoch 4/5 Batch 800/938 Loss: 0.0016\n",
      "  Epoch 4/5 Batch 900/938 Loss: 0.0012\n",
      "MNIST Pre-train Epoch 4/5 complete.\n",
      "  Epoch 5/5 Batch 0/938 Loss: 0.0049\n",
      "  Epoch 5/5 Batch 100/938 Loss: 0.0664\n",
      "  Epoch 5/5 Batch 200/938 Loss: 0.0117\n",
      "  Epoch 5/5 Batch 300/938 Loss: 0.0014\n",
      "  Epoch 5/5 Batch 400/938 Loss: 0.0025\n",
      "  Epoch 5/5 Batch 500/938 Loss: 0.0036\n",
      "  Epoch 5/5 Batch 600/938 Loss: 0.0311\n",
      "  Epoch 5/5 Batch 700/938 Loss: 0.0004\n",
      "  Epoch 5/5 Batch 800/938 Loss: 0.0067\n",
      "  Epoch 5/5 Batch 900/938 Loss: 0.0020\n",
      "MNIST Pre-train Epoch 5/5 complete.\n",
      "Pre-trained MNIST classifier saved to ./experiments\\20251022-140522_N100\\exp_gan_aug_N100\\checkpoints\\classifier_mnist_pretrained.pth\n",
      "run_experiment: Calling _run_gan_aug_scenario with output_dir=./experiments\\20251022-140522_N100\\exp_gan_aug_N100, gan_model_base_dir=./experiments\\20251022-140522_N100\n",
      "  _run_gan_aug_scenario: output_dir=./experiments\\20251022-140522_N100\\exp_gan_aug_N100, gan_model_base_dir=./experiments\\20251022-140522_N100\n",
      "Scenario: Fine-tuning on GAN-augmented SVHN data for 20 epochs...\n",
      "  _run_gan_aug_scenario: gan_load_dir=./experiments\\20251022-140522_N100, gan_model_path=./experiments\\20251022-140522_N100\\checkpoints\\gan_generator_n100.pth\n",
      "Generating 5000 synthetic images using GAN...\n",
      "Synthetic images generated.\n",
      "GAN-augmented data loader created.\n",
      "  Epoch 1/20 Batch 0/80 Loss: 2.5488\n",
      "SVHN GAN Aug Epoch 1/20 complete.\n",
      "  Epoch 2/20 Batch 0/80 Loss: 0.0051\n",
      "SVHN GAN Aug Epoch 2/20 complete.\n",
      "  Epoch 3/20 Batch 0/80 Loss: 0.0486\n",
      "SVHN GAN Aug Epoch 3/20 complete.\n",
      "  Epoch 4/20 Batch 0/80 Loss: 0.0380\n",
      "SVHN GAN Aug Epoch 4/20 complete.\n",
      "  Epoch 5/20 Batch 0/80 Loss: 0.0001\n",
      "SVHN GAN Aug Epoch 5/20 complete.\n",
      "  Epoch 6/20 Batch 0/80 Loss: 0.0005\n",
      "SVHN GAN Aug Epoch 6/20 complete.\n",
      "  Epoch 7/20 Batch 0/80 Loss: 0.0003\n",
      "SVHN GAN Aug Epoch 7/20 complete.\n",
      "  Epoch 8/20 Batch 0/80 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 8/20 complete.\n",
      "  Epoch 9/20 Batch 0/80 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 9/20 complete.\n",
      "  Epoch 10/20 Batch 0/80 Loss: 0.0007\n",
      "SVHN GAN Aug Epoch 10/20 complete.\n",
      "  Epoch 11/20 Batch 0/80 Loss: 0.0003\n",
      "SVHN GAN Aug Epoch 11/20 complete.\n",
      "  Epoch 12/20 Batch 0/80 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 12/20 complete.\n",
      "  Epoch 13/20 Batch 0/80 Loss: 0.0009\n",
      "SVHN GAN Aug Epoch 13/20 complete.\n",
      "  Epoch 14/20 Batch 0/80 Loss: 0.0002\n",
      "SVHN GAN Aug Epoch 14/20 complete.\n",
      "  Epoch 15/20 Batch 0/80 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 15/20 complete.\n",
      "  Epoch 16/20 Batch 0/80 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 16/20 complete.\n",
      "  Epoch 17/20 Batch 0/80 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 17/20 complete.\n",
      "  Epoch 18/20 Batch 0/80 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 18/20 complete.\n",
      "  Epoch 19/20 Batch 0/80 Loss: 0.0000\n",
      "SVHN GAN Aug Epoch 19/20 complete.\n",
      "  Epoch 20/20 Batch 0/80 Loss: 0.0001\n",
      "SVHN GAN Aug Epoch 20/20 complete.\n",
      "GAN-augmented classifier model saved to ./experiments\\20251022-140522_N100\\exp_gan_aug_N100\\checkpoints\\classifier_gan_aug.pth\n",
      "\n",
      "--- Performing Final Evaluation on SVHN Test Set ---\n",
      "\n",
      "--- Results for Scenario: GAN_AUG (N=100) ---\n",
      "Final Accuracy on SVHN Test Set: 40.62%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2384    0.5120    0.3253      1744\n",
      "           1     0.6930    0.5154    0.5912      5099\n",
      "           2     0.6967    0.4666    0.5589      4149\n",
      "           3     0.3736    0.2821    0.3215      2882\n",
      "           4     0.4585    0.2037    0.2821      2523\n",
      "           5     0.3311    0.3721    0.3504      2384\n",
      "           6     0.2527    0.3794    0.3033      1977\n",
      "           7     0.4842    0.4542    0.4687      2019\n",
      "           8     0.1764    0.2614    0.2107      1660\n",
      "           9     0.3314    0.5022    0.3993      1595\n",
      "\n",
      "    accuracy                         0.4062     26032\n",
      "   macro avg     0.4036    0.3949    0.3811     26032\n",
      "weighted avg     0.4672    0.4062    0.4190     26032\n",
      "\n",
      "============================================================\n",
      "Experiment results saved to: ./experiments\\20251022-140522_N100\\exp_gan_aug_N100\\results\\gan_aug_N100_results.json\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-140522_N100\\exp_gan_aug_N100\\tensorboard_logs\n",
      "\n",
      "--- Preparing for experiments with N_samples = 1000 ---\n",
      "Experiment Loop: base_experiment_run_dir=./experiments\\20251022-141123_N1000\n",
      "Experiment Loop: Calling train_gan with output_dir=./experiments\\20251022-141123_N1000\n",
      "train_gan: output_dir=./experiments\\20251022-141123_N1000\n",
      "Starting GAN training on device: cuda:0\n",
      "TensorBoard logs for GAN training will be saved to: ./experiments\\20251022-141123_N1000\\tensorboard_logs\\20251022-141123_GAN_N1000\n",
      "Starting GAN training with 100 epochs on 1000 SVHN samples...\n",
      "Epoch [1/100] Loss_D: 1.0925 Loss_G: 1.7051\n",
      "Epoch [2/100] Loss_D: 1.1821 Loss_G: 2.0788\n",
      "Epoch [3/100] Loss_D: 1.1487 Loss_G: 2.5735\n",
      "Epoch [4/100] Loss_D: 0.9886 Loss_G: 3.8983\n",
      "Epoch [5/100] Loss_D: 1.0738 Loss_G: 2.6909\n",
      "Epoch [6/100] Loss_D: 0.4855 Loss_G: 2.5109\n",
      "Epoch [7/100] Loss_D: 0.6930 Loss_G: 1.9280\n",
      "Epoch [8/100] Loss_D: 0.5969 Loss_G: 2.6512\n",
      "Epoch [9/100] Loss_D: 1.2875 Loss_G: 1.2476\n",
      "Epoch [10/100] Loss_D: 0.7183 Loss_G: 2.0650\n",
      "Saved generated images for epoch 10.\n",
      "Epoch [11/100] Loss_D: 0.5841 Loss_G: 2.5460\n",
      "Epoch [12/100] Loss_D: 0.4732 Loss_G: 2.2369\n",
      "Epoch [13/100] Loss_D: 0.5717 Loss_G: 2.5994\n",
      "Epoch [14/100] Loss_D: 0.9023 Loss_G: 2.2816\n",
      "Epoch [15/100] Loss_D: 0.5372 Loss_G: 2.6355\n",
      "Epoch [16/100] Loss_D: 0.8905 Loss_G: 2.3729\n",
      "Epoch [17/100] Loss_D: 0.8392 Loss_G: 1.9587\n",
      "Epoch [18/100] Loss_D: 0.5234 Loss_G: 2.8964\n",
      "Epoch [19/100] Loss_D: 0.8579 Loss_G: 2.2986\n",
      "Epoch [20/100] Loss_D: 0.7050 Loss_G: 2.6513\n",
      "Saved generated images for epoch 20.\n",
      "Epoch [21/100] Loss_D: 1.1737 Loss_G: 3.5862\n",
      "Epoch [22/100] Loss_D: 0.7663 Loss_G: 1.9785\n",
      "Epoch [23/100] Loss_D: 0.7539 Loss_G: 2.5105\n",
      "Epoch [24/100] Loss_D: 0.5344 Loss_G: 2.8100\n",
      "Epoch [25/100] Loss_D: 0.9817 Loss_G: 2.6859\n",
      "Epoch [26/100] Loss_D: 0.5447 Loss_G: 2.6289\n",
      "Epoch [27/100] Loss_D: 0.8057 Loss_G: 3.0424\n",
      "Epoch [28/100] Loss_D: 0.6280 Loss_G: 1.6543\n",
      "Epoch [29/100] Loss_D: 0.5316 Loss_G: 3.5831\n",
      "Epoch [30/100] Loss_D: 0.7159 Loss_G: 2.9134\n",
      "Saved generated images for epoch 30.\n",
      "Epoch [31/100] Loss_D: 0.4497 Loss_G: 2.4969\n",
      "Epoch [32/100] Loss_D: 0.5577 Loss_G: 2.2734\n",
      "Epoch [33/100] Loss_D: 0.5671 Loss_G: 2.2652\n",
      "Epoch [34/100] Loss_D: 0.8969 Loss_G: 2.4885\n",
      "Epoch [35/100] Loss_D: 0.9671 Loss_G: 2.9434\n",
      "Epoch [36/100] Loss_D: 0.4320 Loss_G: 2.8122\n",
      "Epoch [37/100] Loss_D: 0.6248 Loss_G: 2.1695\n",
      "Epoch [38/100] Loss_D: 0.8573 Loss_G: 2.7256\n",
      "Epoch [39/100] Loss_D: 1.0538 Loss_G: 1.8990\n",
      "Epoch [40/100] Loss_D: 0.2450 Loss_G: 3.5769\n",
      "Saved generated images for epoch 40.\n",
      "Epoch [41/100] Loss_D: 0.8863 Loss_G: 2.0783\n",
      "Epoch [42/100] Loss_D: 1.3176 Loss_G: 3.4134\n",
      "Epoch [43/100] Loss_D: 1.0511 Loss_G: 2.7294\n",
      "Epoch [44/100] Loss_D: 0.7880 Loss_G: 2.3366\n",
      "Epoch [45/100] Loss_D: 0.5482 Loss_G: 3.1216\n",
      "Epoch [46/100] Loss_D: 0.5634 Loss_G: 2.6523\n",
      "Epoch [47/100] Loss_D: 0.6889 Loss_G: 2.7619\n",
      "Epoch [48/100] Loss_D: 0.5489 Loss_G: 2.7830\n",
      "Epoch [49/100] Loss_D: 0.5295 Loss_G: 2.6818\n",
      "Epoch [50/100] Loss_D: 1.0073 Loss_G: 3.2796\n",
      "Saved generated images for epoch 50.\n",
      "Epoch [51/100] Loss_D: 1.0243 Loss_G: 2.7291\n",
      "Epoch [52/100] Loss_D: 0.9239 Loss_G: 1.8934\n",
      "Epoch [53/100] Loss_D: 0.4333 Loss_G: 2.8518\n",
      "Epoch [54/100] Loss_D: 0.7673 Loss_G: 3.1452\n",
      "Epoch [55/100] Loss_D: 0.6990 Loss_G: 2.2109\n",
      "Epoch [56/100] Loss_D: 0.8273 Loss_G: 2.1927\n",
      "Epoch [57/100] Loss_D: 0.5477 Loss_G: 2.0600\n",
      "Epoch [58/100] Loss_D: 0.9385 Loss_G: 1.8604\n",
      "Epoch [59/100] Loss_D: 0.5853 Loss_G: 2.8107\n",
      "Epoch [60/100] Loss_D: 1.0175 Loss_G: 1.8775\n",
      "Saved generated images for epoch 60.\n",
      "Epoch [61/100] Loss_D: 1.0013 Loss_G: 2.8908\n",
      "Epoch [62/100] Loss_D: 0.5251 Loss_G: 2.7728\n",
      "Epoch [63/100] Loss_D: 0.8550 Loss_G: 2.9429\n",
      "Epoch [64/100] Loss_D: 0.8329 Loss_G: 2.5796\n",
      "Epoch [65/100] Loss_D: 0.3627 Loss_G: 3.8510\n",
      "Epoch [66/100] Loss_D: 0.4137 Loss_G: 2.8104\n",
      "Epoch [67/100] Loss_D: 0.8457 Loss_G: 2.3966\n",
      "Epoch [68/100] Loss_D: 0.6204 Loss_G: 2.2512\n",
      "Epoch [69/100] Loss_D: 0.6534 Loss_G: 2.8152\n",
      "Epoch [70/100] Loss_D: 0.8118 Loss_G: 2.2297\n",
      "Saved generated images for epoch 70.\n",
      "Epoch [71/100] Loss_D: 1.7190 Loss_G: 2.8164\n",
      "Epoch [72/100] Loss_D: 0.7414 Loss_G: 2.8558\n",
      "Epoch [73/100] Loss_D: 0.5796 Loss_G: 2.5433\n",
      "Epoch [74/100] Loss_D: 0.8350 Loss_G: 2.6423\n",
      "Epoch [75/100] Loss_D: 0.9952 Loss_G: 1.7855\n",
      "Epoch [76/100] Loss_D: 0.6230 Loss_G: 2.5651\n",
      "Epoch [77/100] Loss_D: 0.5675 Loss_G: 2.8618\n",
      "Epoch [78/100] Loss_D: 0.7510 Loss_G: 2.9495\n",
      "Epoch [79/100] Loss_D: 0.8347 Loss_G: 2.1115\n",
      "Epoch [80/100] Loss_D: 0.4139 Loss_G: 3.9751\n",
      "Saved generated images for epoch 80.\n",
      "Epoch [81/100] Loss_D: 0.8361 Loss_G: 2.9115\n",
      "Epoch [82/100] Loss_D: 0.5670 Loss_G: 2.8221\n",
      "Epoch [83/100] Loss_D: 0.5706 Loss_G: 2.7352\n",
      "Epoch [84/100] Loss_D: 0.7121 Loss_G: 2.3236\n",
      "Epoch [85/100] Loss_D: 0.4992 Loss_G: 2.4256\n",
      "Epoch [86/100] Loss_D: 0.5832 Loss_G: 2.4627\n",
      "Epoch [87/100] Loss_D: 0.5049 Loss_G: 2.5156\n",
      "Epoch [88/100] Loss_D: 0.7309 Loss_G: 2.5219\n",
      "Epoch [89/100] Loss_D: 0.5263 Loss_G: 3.4423\n",
      "Epoch [90/100] Loss_D: 0.4479 Loss_G: 3.5928\n",
      "Saved generated images for epoch 90.\n",
      "Epoch [91/100] Loss_D: 0.8328 Loss_G: 4.6821\n",
      "Epoch [92/100] Loss_D: 0.8234 Loss_G: 2.3917\n",
      "Epoch [93/100] Loss_D: 0.3601 Loss_G: 2.3972\n",
      "Epoch [94/100] Loss_D: 0.6262 Loss_G: 2.4078\n",
      "Epoch [95/100] Loss_D: 0.7491 Loss_G: 1.3347\n",
      "Epoch [96/100] Loss_D: 0.5268 Loss_G: 2.5587\n",
      "Epoch [97/100] Loss_D: 0.4580 Loss_G: 2.2465\n",
      "Epoch [98/100] Loss_D: 0.7363 Loss_G: 2.3911\n",
      "Epoch [99/100] Loss_D: 0.7628 Loss_G: 3.0321\n",
      "Epoch [100/100] Loss_D: 0.7979 Loss_G: 2.1546\n",
      "Saved generated images for epoch 100.\n",
      "Finished GAN Training. Generator model saved to ./experiments\\20251022-141123_N1000\\checkpoints\\gan_generator_n1000.pth\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-141123_N1000\\tensorboard_logs\n",
      "GAN training for n_samples=1000 completed.\n",
      "\n",
      "Running experiment for scenario: source_only with N=1000\n",
      "Experiment Loop: Calling run_experiment for scenario=source_only, n_samples=1000 with args.output_dir=./experiments\\20251022-141123_N1000\\exp_source_only_N1000, gan_model_base_dir=./experiments\\20251022-141123_N1000\n",
      "run_experiment: args.output_dir=./experiments\\20251022-141123_N1000\\exp_source_only_N1000, gan_model_base_dir=./experiments\\20251022-141123_N1000\n",
      "TensorBoard logs will be saved to: ./experiments\\20251022-141123_N1000\\exp_source_only_N1000\\tensorboard_logs\\20251022-141204_source_only_N1000\n",
      "\n",
      "===== RUNNING SCENARIO: SOURCE_ONLY with N=1000 on cuda:0 =====\n",
      "Pre-training classifier on MNIST for 5 epochs...\n",
      "  Epoch 1/5 Batch 0/938 Loss: 2.3019\n",
      "  Epoch 1/5 Batch 100/938 Loss: 0.1048\n",
      "  Epoch 1/5 Batch 200/938 Loss: 0.0396\n",
      "  Epoch 1/5 Batch 300/938 Loss: 0.1445\n",
      "  Epoch 1/5 Batch 400/938 Loss: 0.0872\n",
      "  Epoch 1/5 Batch 500/938 Loss: 0.0840\n",
      "  Epoch 1/5 Batch 600/938 Loss: 0.0080\n",
      "  Epoch 1/5 Batch 700/938 Loss: 0.0158\n",
      "  Epoch 1/5 Batch 800/938 Loss: 0.0213\n",
      "  Epoch 1/5 Batch 900/938 Loss: 0.0833\n",
      "MNIST Pre-train Epoch 1/5 complete.\n",
      "  Epoch 2/5 Batch 0/938 Loss: 0.0186\n",
      "  Epoch 2/5 Batch 100/938 Loss: 0.0043\n",
      "  Epoch 2/5 Batch 200/938 Loss: 0.0200\n",
      "  Epoch 2/5 Batch 300/938 Loss: 0.0191\n",
      "  Epoch 2/5 Batch 400/938 Loss: 0.0144\n",
      "  Epoch 2/5 Batch 500/938 Loss: 0.0259\n",
      "  Epoch 2/5 Batch 600/938 Loss: 0.0095\n",
      "  Epoch 2/5 Batch 700/938 Loss: 0.0751\n",
      "  Epoch 2/5 Batch 800/938 Loss: 0.0080\n",
      "  Epoch 2/5 Batch 900/938 Loss: 0.1030\n",
      "MNIST Pre-train Epoch 2/5 complete.\n",
      "  Epoch 3/5 Batch 0/938 Loss: 0.0287\n",
      "  Epoch 3/5 Batch 100/938 Loss: 0.0821\n",
      "  Epoch 3/5 Batch 200/938 Loss: 0.0098\n",
      "  Epoch 3/5 Batch 300/938 Loss: 0.0002\n",
      "  Epoch 3/5 Batch 400/938 Loss: 0.2621\n",
      "  Epoch 3/5 Batch 500/938 Loss: 0.0061\n",
      "  Epoch 3/5 Batch 600/938 Loss: 0.0721\n",
      "  Epoch 3/5 Batch 700/938 Loss: 0.0025\n",
      "  Epoch 3/5 Batch 800/938 Loss: 0.2934\n",
      "  Epoch 3/5 Batch 900/938 Loss: 0.1316\n",
      "MNIST Pre-train Epoch 3/5 complete.\n",
      "  Epoch 4/5 Batch 0/938 Loss: 0.0059\n",
      "  Epoch 4/5 Batch 100/938 Loss: 0.0008\n",
      "  Epoch 4/5 Batch 200/938 Loss: 0.0631\n",
      "  Epoch 4/5 Batch 300/938 Loss: 0.0095\n",
      "  Epoch 4/5 Batch 400/938 Loss: 0.0328\n",
      "  Epoch 4/5 Batch 500/938 Loss: 0.0198\n",
      "  Epoch 4/5 Batch 600/938 Loss: 0.0261\n",
      "  Epoch 4/5 Batch 700/938 Loss: 0.0064\n",
      "  Epoch 4/5 Batch 800/938 Loss: 0.0104\n",
      "  Epoch 4/5 Batch 900/938 Loss: 0.0209\n",
      "MNIST Pre-train Epoch 4/5 complete.\n",
      "  Epoch 5/5 Batch 0/938 Loss: 0.0037\n",
      "  Epoch 5/5 Batch 100/938 Loss: 0.0159\n",
      "  Epoch 5/5 Batch 200/938 Loss: 0.0000\n",
      "  Epoch 5/5 Batch 300/938 Loss: 0.0014\n",
      "  Epoch 5/5 Batch 400/938 Loss: 0.0010\n",
      "  Epoch 5/5 Batch 500/938 Loss: 0.0007\n",
      "  Epoch 5/5 Batch 600/938 Loss: 0.0125\n",
      "  Epoch 5/5 Batch 700/938 Loss: 0.0180\n",
      "  Epoch 5/5 Batch 800/938 Loss: 0.0068\n",
      "  Epoch 5/5 Batch 900/938 Loss: 0.0127\n",
      "MNIST Pre-train Epoch 5/5 complete.\n",
      "Pre-trained MNIST classifier saved to ./experiments\\20251022-141123_N1000\\exp_source_only_N1000\\checkpoints\\classifier_mnist_pretrained.pth\n",
      "Scenario: Source Only - Evaluating MNIST pre-trained model directly on SVHN.\n",
      "Source-only classifier model saved to ./experiments\\20251022-141123_N1000\\exp_source_only_N1000\\checkpoints\\classifier_source_only.pth\n",
      "\n",
      "--- Performing Final Evaluation on SVHN Test Set ---\n",
      "\n",
      "--- Results for Scenario: SOURCE_ONLY (N=1000) ---\n",
      "Final Accuracy on SVHN Test Set: 27.59%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2614    0.1416    0.1837      1744\n",
      "           1     0.3816    0.2969    0.3340      5099\n",
      "           2     0.5516    0.3013    0.3897      4149\n",
      "           3     0.3397    0.1978    0.2500      2882\n",
      "           4     0.1340    0.3928    0.1998      2523\n",
      "           5     0.2799    0.4216    0.3365      2384\n",
      "           6     0.2722    0.1300    0.1760      1977\n",
      "           7     0.2620    0.3680    0.3061      2019\n",
      "           8     0.3446    0.1717    0.2292      1660\n",
      "           9     0.2018    0.2000    0.2009      1595\n",
      "\n",
      "    accuracy                         0.2759     26032\n",
      "   macro avg     0.3029    0.2622    0.2606     26032\n",
      "weighted avg     0.3317    0.2759    0.2817     26032\n",
      "\n",
      "============================================================\n",
      "Experiment results saved to: ./experiments\\20251022-141123_N1000\\exp_source_only_N1000\\results\\source_only_N1000_results.json\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-141123_N1000\\exp_source_only_N1000\\tensorboard_logs\n",
      "\n",
      "Running experiment for scenario: fine_tune with N=1000\n",
      "Experiment Loop: Calling run_experiment for scenario=fine_tune, n_samples=1000 with args.output_dir=./experiments\\20251022-141123_N1000\\exp_fine_tune_N1000, gan_model_base_dir=./experiments\\20251022-141123_N1000\n",
      "run_experiment: args.output_dir=./experiments\\20251022-141123_N1000\\exp_fine_tune_N1000, gan_model_base_dir=./experiments\\20251022-141123_N1000\n",
      "TensorBoard logs will be saved to: ./experiments\\20251022-141123_N1000\\exp_fine_tune_N1000\\tensorboard_logs\\20251022-141337_fine_tune_N1000\n",
      "\n",
      "===== RUNNING SCENARIO: FINE_TUNE with N=1000 on cuda:0 =====\n",
      "Pre-training classifier on MNIST for 5 epochs...\n",
      "  Epoch 1/5 Batch 0/938 Loss: 2.3073\n",
      "  Epoch 1/5 Batch 100/938 Loss: 0.2855\n",
      "  Epoch 1/5 Batch 200/938 Loss: 0.0988\n",
      "  Epoch 1/5 Batch 300/938 Loss: 0.1252\n",
      "  Epoch 1/5 Batch 400/938 Loss: 0.0717\n",
      "  Epoch 1/5 Batch 500/938 Loss: 0.1219\n",
      "  Epoch 1/5 Batch 600/938 Loss: 0.0498\n",
      "  Epoch 1/5 Batch 700/938 Loss: 0.0244\n",
      "  Epoch 1/5 Batch 800/938 Loss: 0.0995\n",
      "  Epoch 1/5 Batch 900/938 Loss: 0.0305\n",
      "MNIST Pre-train Epoch 1/5 complete.\n",
      "  Epoch 2/5 Batch 0/938 Loss: 0.0328\n",
      "  Epoch 2/5 Batch 100/938 Loss: 0.0263\n",
      "  Epoch 2/5 Batch 200/938 Loss: 0.0458\n",
      "  Epoch 2/5 Batch 300/938 Loss: 0.0078\n",
      "  Epoch 2/5 Batch 400/938 Loss: 0.0398\n",
      "  Epoch 2/5 Batch 500/938 Loss: 0.1445\n",
      "  Epoch 2/5 Batch 600/938 Loss: 0.0706\n",
      "  Epoch 2/5 Batch 700/938 Loss: 0.0065\n",
      "  Epoch 2/5 Batch 800/938 Loss: 0.0020\n",
      "  Epoch 2/5 Batch 900/938 Loss: 0.0263\n",
      "MNIST Pre-train Epoch 2/5 complete.\n",
      "  Epoch 3/5 Batch 0/938 Loss: 0.0043\n",
      "  Epoch 3/5 Batch 100/938 Loss: 0.0010\n",
      "  Epoch 3/5 Batch 200/938 Loss: 0.0816\n",
      "  Epoch 3/5 Batch 300/938 Loss: 0.0042\n",
      "  Epoch 3/5 Batch 400/938 Loss: 0.0064\n",
      "  Epoch 3/5 Batch 500/938 Loss: 0.0388\n",
      "  Epoch 3/5 Batch 600/938 Loss: 0.0308\n",
      "  Epoch 3/5 Batch 700/938 Loss: 0.0232\n",
      "  Epoch 3/5 Batch 800/938 Loss: 0.0043\n",
      "  Epoch 3/5 Batch 900/938 Loss: 0.0123\n",
      "MNIST Pre-train Epoch 3/5 complete.\n",
      "  Epoch 4/5 Batch 0/938 Loss: 0.0100\n",
      "  Epoch 4/5 Batch 100/938 Loss: 0.0119\n",
      "  Epoch 4/5 Batch 200/938 Loss: 0.0045\n",
      "  Epoch 4/5 Batch 300/938 Loss: 0.1005\n",
      "  Epoch 4/5 Batch 400/938 Loss: 0.0052\n",
      "  Epoch 4/5 Batch 500/938 Loss: 0.0015\n",
      "  Epoch 4/5 Batch 600/938 Loss: 0.0179\n",
      "  Epoch 4/5 Batch 700/938 Loss: 0.0957\n",
      "  Epoch 4/5 Batch 800/938 Loss: 0.0633\n",
      "  Epoch 4/5 Batch 900/938 Loss: 0.0044\n",
      "MNIST Pre-train Epoch 4/5 complete.\n",
      "  Epoch 5/5 Batch 0/938 Loss: 0.0006\n",
      "  Epoch 5/5 Batch 100/938 Loss: 0.0251\n",
      "  Epoch 5/5 Batch 200/938 Loss: 0.0011\n",
      "  Epoch 5/5 Batch 300/938 Loss: 0.0035\n",
      "  Epoch 5/5 Batch 400/938 Loss: 0.0916\n",
      "  Epoch 5/5 Batch 500/938 Loss: 0.0692\n",
      "  Epoch 5/5 Batch 600/938 Loss: 0.0002\n",
      "  Epoch 5/5 Batch 700/938 Loss: 0.0128\n",
      "  Epoch 5/5 Batch 800/938 Loss: 0.0048\n",
      "  Epoch 5/5 Batch 900/938 Loss: 0.0020\n",
      "MNIST Pre-train Epoch 5/5 complete.\n",
      "Pre-trained MNIST classifier saved to ./experiments\\20251022-141123_N1000\\exp_fine_tune_N1000\\checkpoints\\classifier_mnist_pretrained.pth\n",
      "Scenario: Fine-tuning on low-resource SVHN data for 20 epochs...\n",
      "  Epoch 1/20 Batch 0/16 Loss: 2.5317\n",
      "SVHN Fine-tune Epoch 1/20 complete.\n",
      "  Epoch 2/20 Batch 0/16 Loss: 1.4354\n",
      "SVHN Fine-tune Epoch 2/20 complete.\n",
      "  Epoch 3/20 Batch 0/16 Loss: 0.9372\n",
      "SVHN Fine-tune Epoch 3/20 complete.\n",
      "  Epoch 4/20 Batch 0/16 Loss: 0.5170\n",
      "SVHN Fine-tune Epoch 4/20 complete.\n",
      "  Epoch 5/20 Batch 0/16 Loss: 0.3520\n",
      "SVHN Fine-tune Epoch 5/20 complete.\n",
      "  Epoch 6/20 Batch 0/16 Loss: 0.1913\n",
      "SVHN Fine-tune Epoch 6/20 complete.\n",
      "  Epoch 7/20 Batch 0/16 Loss: 0.1532\n",
      "SVHN Fine-tune Epoch 7/20 complete.\n",
      "  Epoch 8/20 Batch 0/16 Loss: 0.1116\n",
      "SVHN Fine-tune Epoch 8/20 complete.\n",
      "  Epoch 9/20 Batch 0/16 Loss: 0.0452\n",
      "SVHN Fine-tune Epoch 9/20 complete.\n",
      "  Epoch 10/20 Batch 0/16 Loss: 0.0290\n",
      "SVHN Fine-tune Epoch 10/20 complete.\n",
      "  Epoch 11/20 Batch 0/16 Loss: 0.0374\n",
      "SVHN Fine-tune Epoch 11/20 complete.\n",
      "  Epoch 12/20 Batch 0/16 Loss: 0.0157\n",
      "SVHN Fine-tune Epoch 12/20 complete.\n",
      "  Epoch 13/20 Batch 0/16 Loss: 0.0250\n",
      "SVHN Fine-tune Epoch 13/20 complete.\n",
      "  Epoch 14/20 Batch 0/16 Loss: 0.0265\n",
      "SVHN Fine-tune Epoch 14/20 complete.\n",
      "  Epoch 15/20 Batch 0/16 Loss: 0.0106\n",
      "SVHN Fine-tune Epoch 15/20 complete.\n",
      "  Epoch 16/20 Batch 0/16 Loss: 0.0065\n",
      "SVHN Fine-tune Epoch 16/20 complete.\n",
      "  Epoch 17/20 Batch 0/16 Loss: 0.0117\n",
      "SVHN Fine-tune Epoch 17/20 complete.\n",
      "  Epoch 18/20 Batch 0/16 Loss: 0.0034\n",
      "SVHN Fine-tune Epoch 18/20 complete.\n",
      "  Epoch 19/20 Batch 0/16 Loss: 0.0036\n",
      "SVHN Fine-tune Epoch 19/20 complete.\n",
      "  Epoch 20/20 Batch 0/16 Loss: 0.0037\n",
      "SVHN Fine-tune Epoch 20/20 complete.\n",
      "Fine-tuned classifier model saved to ./experiments\\20251022-141123_N1000\\exp_fine_tune_N1000\\checkpoints\\classifier_fine_tune.pth\n",
      "\n",
      "--- Performing Final Evaluation on SVHN Test Set ---\n",
      "\n",
      "--- Results for Scenario: FINE_TUNE (N=1000) ---\n",
      "Final Accuracy on SVHN Test Set: 74.98%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7720    0.7689    0.7705      1744\n",
      "           1     0.8883    0.8127    0.8488      5099\n",
      "           2     0.8663    0.7855    0.8239      4149\n",
      "           3     0.7027    0.6322    0.6656      2882\n",
      "           4     0.7459    0.7923    0.7684      2523\n",
      "           5     0.6526    0.7911    0.7152      2384\n",
      "           6     0.7273    0.6717    0.6984      1977\n",
      "           7     0.6269    0.8762    0.7308      2019\n",
      "           8     0.6347    0.5380    0.5823      1660\n",
      "           9     0.6527    0.6752    0.6638      1595\n",
      "\n",
      "    accuracy                         0.7498     26032\n",
      "   macro avg     0.7269    0.7344    0.7268     26032\n",
      "weighted avg     0.7580    0.7498    0.7504     26032\n",
      "\n",
      "============================================================\n",
      "Experiment results saved to: ./experiments\\20251022-141123_N1000\\exp_fine_tune_N1000\\results\\fine_tune_N1000_results.json\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-141123_N1000\\exp_fine_tune_N1000\\tensorboard_logs\n",
      "\n",
      "Running experiment for scenario: traditional_aug with N=1000\n",
      "Experiment Loop: Calling run_experiment for scenario=traditional_aug, n_samples=1000 with args.output_dir=./experiments\\20251022-141123_N1000\\exp_traditional_aug_N1000, gan_model_base_dir=./experiments\\20251022-141123_N1000\n",
      "run_experiment: args.output_dir=./experiments\\20251022-141123_N1000\\exp_traditional_aug_N1000, gan_model_base_dir=./experiments\\20251022-141123_N1000\n",
      "TensorBoard logs will be saved to: ./experiments\\20251022-141123_N1000\\exp_traditional_aug_N1000\\tensorboard_logs\\20251022-141517_traditional_aug_N1000\n",
      "\n",
      "===== RUNNING SCENARIO: TRADITIONAL_AUG with N=1000 on cuda:0 =====\n",
      "Pre-training classifier on MNIST for 5 epochs...\n",
      "  Epoch 1/5 Batch 0/938 Loss: 2.3222\n",
      "  Epoch 1/5 Batch 100/938 Loss: 0.2112\n",
      "  Epoch 1/5 Batch 200/938 Loss: 0.0690\n",
      "  Epoch 1/5 Batch 300/938 Loss: 0.0788\n",
      "  Epoch 1/5 Batch 400/938 Loss: 0.0892\n",
      "  Epoch 1/5 Batch 500/938 Loss: 0.0303\n",
      "  Epoch 1/5 Batch 600/938 Loss: 0.0118\n",
      "  Epoch 1/5 Batch 700/938 Loss: 0.0218\n",
      "  Epoch 1/5 Batch 800/938 Loss: 0.1977\n",
      "  Epoch 1/5 Batch 900/938 Loss: 0.1147\n",
      "MNIST Pre-train Epoch 1/5 complete.\n",
      "  Epoch 2/5 Batch 0/938 Loss: 0.1107\n",
      "  Epoch 2/5 Batch 100/938 Loss: 0.0175\n",
      "  Epoch 2/5 Batch 200/938 Loss: 0.0964\n",
      "  Epoch 2/5 Batch 300/938 Loss: 0.0149\n",
      "  Epoch 2/5 Batch 400/938 Loss: 0.0624\n",
      "  Epoch 2/5 Batch 500/938 Loss: 0.0134\n",
      "  Epoch 2/5 Batch 600/938 Loss: 0.0508\n",
      "  Epoch 2/5 Batch 700/938 Loss: 0.0511\n",
      "  Epoch 2/5 Batch 800/938 Loss: 0.0738\n",
      "  Epoch 2/5 Batch 900/938 Loss: 0.0289\n",
      "MNIST Pre-train Epoch 2/5 complete.\n",
      "  Epoch 3/5 Batch 0/938 Loss: 0.0751\n",
      "  Epoch 3/5 Batch 100/938 Loss: 0.1130\n",
      "  Epoch 3/5 Batch 200/938 Loss: 0.0062\n",
      "  Epoch 3/5 Batch 300/938 Loss: 0.0184\n",
      "  Epoch 3/5 Batch 400/938 Loss: 0.0027\n",
      "  Epoch 3/5 Batch 500/938 Loss: 0.0319\n",
      "  Epoch 3/5 Batch 600/938 Loss: 0.0635\n",
      "  Epoch 3/5 Batch 700/938 Loss: 0.0106\n",
      "  Epoch 3/5 Batch 800/938 Loss: 0.0165\n",
      "  Epoch 3/5 Batch 900/938 Loss: 0.0031\n",
      "MNIST Pre-train Epoch 3/5 complete.\n",
      "  Epoch 4/5 Batch 0/938 Loss: 0.0475\n",
      "  Epoch 4/5 Batch 100/938 Loss: 0.0123\n",
      "  Epoch 4/5 Batch 200/938 Loss: 0.0158\n",
      "  Epoch 4/5 Batch 300/938 Loss: 0.0017\n",
      "  Epoch 4/5 Batch 400/938 Loss: 0.1367\n",
      "  Epoch 4/5 Batch 500/938 Loss: 0.0405\n",
      "  Epoch 4/5 Batch 600/938 Loss: 0.0257\n",
      "  Epoch 4/5 Batch 700/938 Loss: 0.0237\n",
      "  Epoch 4/5 Batch 800/938 Loss: 0.0113\n",
      "  Epoch 4/5 Batch 900/938 Loss: 0.0604\n",
      "MNIST Pre-train Epoch 4/5 complete.\n",
      "  Epoch 5/5 Batch 0/938 Loss: 0.1649\n",
      "  Epoch 5/5 Batch 100/938 Loss: 0.0092\n",
      "  Epoch 5/5 Batch 200/938 Loss: 0.0581\n",
      "  Epoch 5/5 Batch 300/938 Loss: 0.0047\n",
      "  Epoch 5/5 Batch 400/938 Loss: 0.0031\n",
      "  Epoch 5/5 Batch 500/938 Loss: 0.0017\n",
      "  Epoch 5/5 Batch 600/938 Loss: 0.0058\n",
      "  Epoch 5/5 Batch 700/938 Loss: 0.0098\n",
      "  Epoch 5/5 Batch 800/938 Loss: 0.1114\n",
      "  Epoch 5/5 Batch 900/938 Loss: 0.0530\n",
      "MNIST Pre-train Epoch 5/5 complete.\n",
      "Pre-trained MNIST classifier saved to ./experiments\\20251022-141123_N1000\\exp_traditional_aug_N1000\\checkpoints\\classifier_mnist_pretrained.pth\n",
      "Scenario: Fine-tuning on traditionally augmented low-resource SVHN data for 20 epochs...\n",
      "  Epoch 1/20 Batch 0/16 Loss: 2.5226\n",
      "SVHN Traditional Aug Epoch 1/20 complete.\n",
      "  Epoch 2/20 Batch 0/16 Loss: 2.1546\n",
      "SVHN Traditional Aug Epoch 2/20 complete.\n",
      "  Epoch 3/20 Batch 0/16 Loss: 1.7716\n",
      "SVHN Traditional Aug Epoch 3/20 complete.\n",
      "  Epoch 4/20 Batch 0/16 Loss: 1.4474\n",
      "SVHN Traditional Aug Epoch 4/20 complete.\n",
      "  Epoch 5/20 Batch 0/16 Loss: 1.2702\n",
      "SVHN Traditional Aug Epoch 5/20 complete.\n",
      "  Epoch 6/20 Batch 0/16 Loss: 1.2645\n",
      "SVHN Traditional Aug Epoch 6/20 complete.\n",
      "  Epoch 7/20 Batch 0/16 Loss: 1.0456\n",
      "SVHN Traditional Aug Epoch 7/20 complete.\n",
      "  Epoch 8/20 Batch 0/16 Loss: 1.1969\n",
      "SVHN Traditional Aug Epoch 8/20 complete.\n",
      "  Epoch 9/20 Batch 0/16 Loss: 0.9551\n",
      "SVHN Traditional Aug Epoch 9/20 complete.\n",
      "  Epoch 10/20 Batch 0/16 Loss: 0.9529\n",
      "SVHN Traditional Aug Epoch 10/20 complete.\n",
      "  Epoch 11/20 Batch 0/16 Loss: 0.8725\n",
      "SVHN Traditional Aug Epoch 11/20 complete.\n",
      "  Epoch 12/20 Batch 0/16 Loss: 0.6659\n",
      "SVHN Traditional Aug Epoch 12/20 complete.\n",
      "  Epoch 13/20 Batch 0/16 Loss: 0.5216\n",
      "SVHN Traditional Aug Epoch 13/20 complete.\n",
      "  Epoch 14/20 Batch 0/16 Loss: 0.7160\n",
      "SVHN Traditional Aug Epoch 14/20 complete.\n",
      "  Epoch 15/20 Batch 0/16 Loss: 0.7261\n",
      "SVHN Traditional Aug Epoch 15/20 complete.\n",
      "  Epoch 16/20 Batch 0/16 Loss: 0.6988\n",
      "SVHN Traditional Aug Epoch 16/20 complete.\n",
      "  Epoch 17/20 Batch 0/16 Loss: 0.5475\n",
      "SVHN Traditional Aug Epoch 17/20 complete.\n",
      "  Epoch 18/20 Batch 0/16 Loss: 0.6052\n",
      "SVHN Traditional Aug Epoch 18/20 complete.\n",
      "  Epoch 19/20 Batch 0/16 Loss: 0.4115\n",
      "SVHN Traditional Aug Epoch 19/20 complete.\n",
      "  Epoch 20/20 Batch 0/16 Loss: 0.3808\n",
      "SVHN Traditional Aug Epoch 20/20 complete.\n",
      "Traditional augmented classifier model saved to ./experiments\\20251022-141123_N1000\\exp_traditional_aug_N1000\\checkpoints\\classifier_traditional_aug.pth\n",
      "\n",
      "--- Performing Final Evaluation on SVHN Test Set ---\n",
      "\n",
      "--- Results for Scenario: TRADITIONAL_AUG (N=1000) ---\n",
      "Final Accuracy on SVHN Test Set: 77.00%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6369    0.8068    0.7119      1744\n",
      "           1     0.9132    0.8155    0.8616      5099\n",
      "           2     0.9318    0.7414    0.8258      4149\n",
      "           3     0.8029    0.6249    0.7028      2882\n",
      "           4     0.7524    0.8601    0.8027      2523\n",
      "           5     0.7279    0.8339    0.7773      2384\n",
      "           6     0.7396    0.7112    0.7251      1977\n",
      "           7     0.7475    0.8519    0.7963      2019\n",
      "           8     0.6631    0.5904    0.6246      1660\n",
      "           9     0.5508    0.8395    0.6652      1595\n",
      "\n",
      "    accuracy                         0.7700     26032\n",
      "   macro avg     0.7466    0.7675    0.7493     26032\n",
      "weighted avg     0.7887    0.7700    0.7723     26032\n",
      "\n",
      "============================================================\n",
      "Experiment results saved to: ./experiments\\20251022-141123_N1000\\exp_traditional_aug_N1000\\results\\traditional_aug_N1000_results.json\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-141123_N1000\\exp_traditional_aug_N1000\\tensorboard_logs\n",
      "\n",
      "Running experiment for scenario: gan_aug with N=1000\n",
      "Experiment Loop: Calling run_experiment for scenario=gan_aug, n_samples=1000 with args.output_dir=./experiments\\20251022-141123_N1000\\exp_gan_aug_N1000, gan_model_base_dir=./experiments\\20251022-141123_N1000\n",
      "run_experiment: args.output_dir=./experiments\\20251022-141123_N1000\\exp_gan_aug_N1000, gan_model_base_dir=./experiments\\20251022-141123_N1000\n",
      "TensorBoard logs will be saved to: ./experiments\\20251022-141123_N1000\\exp_gan_aug_N1000\\tensorboard_logs\\20251022-141652_gan_aug_N1000\n",
      "\n",
      "===== RUNNING SCENARIO: GAN_AUG with N=1000 on cuda:0 =====\n",
      "Pre-training classifier on MNIST for 5 epochs...\n",
      "  Epoch 1/5 Batch 0/938 Loss: 2.3014\n",
      "  Epoch 1/5 Batch 100/938 Loss: 0.2579\n",
      "  Epoch 1/5 Batch 200/938 Loss: 0.1035\n",
      "  Epoch 1/5 Batch 300/938 Loss: 0.0249\n",
      "  Epoch 1/5 Batch 400/938 Loss: 0.0496\n",
      "  Epoch 1/5 Batch 500/938 Loss: 0.0138\n",
      "  Epoch 1/5 Batch 600/938 Loss: 0.0467\n",
      "  Epoch 1/5 Batch 700/938 Loss: 0.1173\n",
      "  Epoch 1/5 Batch 800/938 Loss: 0.0034\n",
      "  Epoch 1/5 Batch 900/938 Loss: 0.0402\n",
      "MNIST Pre-train Epoch 1/5 complete.\n",
      "  Epoch 2/5 Batch 0/938 Loss: 0.0122\n",
      "  Epoch 2/5 Batch 100/938 Loss: 0.0164\n",
      "  Epoch 2/5 Batch 200/938 Loss: 0.0095\n",
      "  Epoch 2/5 Batch 300/938 Loss: 0.0632\n",
      "  Epoch 2/5 Batch 400/938 Loss: 0.0237\n",
      "  Epoch 2/5 Batch 500/938 Loss: 0.1213\n",
      "  Epoch 2/5 Batch 600/938 Loss: 0.0400\n",
      "  Epoch 2/5 Batch 700/938 Loss: 0.0637\n",
      "  Epoch 2/5 Batch 800/938 Loss: 0.0067\n",
      "  Epoch 2/5 Batch 900/938 Loss: 0.0058\n",
      "MNIST Pre-train Epoch 2/5 complete.\n",
      "  Epoch 3/5 Batch 0/938 Loss: 0.0116\n",
      "  Epoch 3/5 Batch 100/938 Loss: 0.0590\n",
      "  Epoch 3/5 Batch 200/938 Loss: 0.0022\n",
      "  Epoch 3/5 Batch 300/938 Loss: 0.0171\n",
      "  Epoch 3/5 Batch 400/938 Loss: 0.0376\n",
      "  Epoch 3/5 Batch 500/938 Loss: 0.0591\n",
      "  Epoch 3/5 Batch 600/938 Loss: 0.0430\n",
      "  Epoch 3/5 Batch 700/938 Loss: 0.1029\n",
      "  Epoch 3/5 Batch 800/938 Loss: 0.0032\n",
      "  Epoch 3/5 Batch 900/938 Loss: 0.0658\n",
      "MNIST Pre-train Epoch 3/5 complete.\n",
      "  Epoch 4/5 Batch 0/938 Loss: 0.0022\n",
      "  Epoch 4/5 Batch 100/938 Loss: 0.0010\n",
      "  Epoch 4/5 Batch 200/938 Loss: 0.0261\n",
      "  Epoch 4/5 Batch 300/938 Loss: 0.0019\n",
      "  Epoch 4/5 Batch 400/938 Loss: 0.0027\n",
      "  Epoch 4/5 Batch 500/938 Loss: 0.0471\n",
      "  Epoch 4/5 Batch 600/938 Loss: 0.0007\n",
      "  Epoch 4/5 Batch 700/938 Loss: 0.0015\n",
      "  Epoch 4/5 Batch 800/938 Loss: 0.0012\n",
      "  Epoch 4/5 Batch 900/938 Loss: 0.0062\n",
      "MNIST Pre-train Epoch 4/5 complete.\n",
      "  Epoch 5/5 Batch 0/938 Loss: 0.0061\n",
      "  Epoch 5/5 Batch 100/938 Loss: 0.0369\n",
      "  Epoch 5/5 Batch 200/938 Loss: 0.0115\n",
      "  Epoch 5/5 Batch 300/938 Loss: 0.0036\n",
      "  Epoch 5/5 Batch 400/938 Loss: 0.0026\n",
      "  Epoch 5/5 Batch 500/938 Loss: 0.0004\n",
      "  Epoch 5/5 Batch 600/938 Loss: 0.0041\n",
      "  Epoch 5/5 Batch 700/938 Loss: 0.0025\n",
      "  Epoch 5/5 Batch 800/938 Loss: 0.0004\n",
      "  Epoch 5/5 Batch 900/938 Loss: 0.0723\n",
      "MNIST Pre-train Epoch 5/5 complete.\n",
      "Pre-trained MNIST classifier saved to ./experiments\\20251022-141123_N1000\\exp_gan_aug_N1000\\checkpoints\\classifier_mnist_pretrained.pth\n",
      "run_experiment: Calling _run_gan_aug_scenario with output_dir=./experiments\\20251022-141123_N1000\\exp_gan_aug_N1000, gan_model_base_dir=./experiments\\20251022-141123_N1000\n",
      "  _run_gan_aug_scenario: output_dir=./experiments\\20251022-141123_N1000\\exp_gan_aug_N1000, gan_model_base_dir=./experiments\\20251022-141123_N1000\n",
      "Scenario: Fine-tuning on GAN-augmented SVHN data for 20 epochs...\n",
      "  _run_gan_aug_scenario: gan_load_dir=./experiments\\20251022-141123_N1000, gan_model_path=./experiments\\20251022-141123_N1000\\checkpoints\\gan_generator_n1000.pth\n",
      "Generating 5000 synthetic images using GAN...\n",
      "Synthetic images generated.\n",
      "GAN-augmented data loader created.\n",
      "  Epoch 1/20 Batch 0/94 Loss: 2.3672\n",
      "SVHN GAN Aug Epoch 1/20 complete.\n",
      "  Epoch 2/20 Batch 0/94 Loss: 0.2056\n",
      "SVHN GAN Aug Epoch 2/20 complete.\n",
      "  Epoch 3/20 Batch 0/94 Loss: 0.0607\n",
      "SVHN GAN Aug Epoch 3/20 complete.\n",
      "  Epoch 4/20 Batch 0/94 Loss: 0.0401\n",
      "SVHN GAN Aug Epoch 4/20 complete.\n",
      "  Epoch 5/20 Batch 0/94 Loss: 0.0318\n",
      "SVHN GAN Aug Epoch 5/20 complete.\n",
      "  Epoch 6/20 Batch 0/94 Loss: 0.0088\n",
      "SVHN GAN Aug Epoch 6/20 complete.\n",
      "  Epoch 7/20 Batch 0/94 Loss: 0.0081\n",
      "SVHN GAN Aug Epoch 7/20 complete.\n",
      "  Epoch 8/20 Batch 0/94 Loss: 0.0161\n",
      "SVHN GAN Aug Epoch 8/20 complete.\n",
      "  Epoch 9/20 Batch 0/94 Loss: 0.0008\n",
      "SVHN GAN Aug Epoch 9/20 complete.\n",
      "  Epoch 10/20 Batch 0/94 Loss: 0.0096\n",
      "SVHN GAN Aug Epoch 10/20 complete.\n",
      "  Epoch 11/20 Batch 0/94 Loss: 0.0041\n",
      "SVHN GAN Aug Epoch 11/20 complete.\n",
      "  Epoch 12/20 Batch 0/94 Loss: 0.0344\n",
      "SVHN GAN Aug Epoch 12/20 complete.\n",
      "  Epoch 13/20 Batch 0/94 Loss: 0.0008\n",
      "SVHN GAN Aug Epoch 13/20 complete.\n",
      "  Epoch 14/20 Batch 0/94 Loss: 0.0069\n",
      "SVHN GAN Aug Epoch 14/20 complete.\n",
      "  Epoch 15/20 Batch 0/94 Loss: 0.0019\n",
      "SVHN GAN Aug Epoch 15/20 complete.\n",
      "  Epoch 16/20 Batch 0/94 Loss: 0.0006\n",
      "SVHN GAN Aug Epoch 16/20 complete.\n",
      "  Epoch 17/20 Batch 0/94 Loss: 0.0002\n",
      "SVHN GAN Aug Epoch 17/20 complete.\n",
      "  Epoch 18/20 Batch 0/94 Loss: 0.0003\n",
      "SVHN GAN Aug Epoch 18/20 complete.\n",
      "  Epoch 19/20 Batch 0/94 Loss: 0.0761\n",
      "SVHN GAN Aug Epoch 19/20 complete.\n",
      "  Epoch 20/20 Batch 0/94 Loss: 0.0079\n",
      "SVHN GAN Aug Epoch 20/20 complete.\n",
      "GAN-augmented classifier model saved to ./experiments\\20251022-141123_N1000\\exp_gan_aug_N1000\\checkpoints\\classifier_gan_aug.pth\n",
      "\n",
      "--- Performing Final Evaluation on SVHN Test Set ---\n",
      "\n",
      "--- Results for Scenario: GAN_AUG (N=1000) ---\n",
      "Final Accuracy on SVHN Test Set: 72.77%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6574    0.7724    0.7103      1744\n",
      "           1     0.8567    0.7621    0.8066      5099\n",
      "           2     0.8590    0.7648    0.8091      4149\n",
      "           3     0.6895    0.6211    0.6535      2882\n",
      "           4     0.6664    0.7895    0.7228      2523\n",
      "           5     0.7138    0.7659    0.7390      2384\n",
      "           6     0.6096    0.6722    0.6394      1977\n",
      "           7     0.7350    0.7801    0.7568      2019\n",
      "           8     0.6636    0.5241    0.5857      1660\n",
      "           9     0.5850    0.7248    0.6474      1595\n",
      "\n",
      "    accuracy                         0.7277     26032\n",
      "   macro avg     0.7036    0.7177    0.7071     26032\n",
      "weighted avg     0.7365    0.7277    0.7289     26032\n",
      "\n",
      "============================================================\n",
      "Experiment results saved to: ./experiments\\20251022-141123_N1000\\exp_gan_aug_N1000\\results\\gan_aug_N1000_results.json\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir ./experiments\\20251022-141123_N1000\\exp_gan_aug_N1000\\tensorboard_logs\n",
      "\n",
      "All experiments completed.\n"
     ]
    }
   ],
   "source": [
    "# --- Experiment Configuration and Execution ---\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.scenario = \"\"\n",
    "        self.n_samples = 100\n",
    "        self.batch_size = 64\n",
    "        self.classifier_lr = 1e-3\n",
    "        self.classifier_epochs_mnist = 5\n",
    "        self.classifier_epochs_finetune = 20\n",
    "        self.gan_nz = 100\n",
    "        self.gan_ngf = 64\n",
    "        self.gan_ndf = 64\n",
    "        self.gan_nc = 1\n",
    "        self.gan_n_classes = 10\n",
    "        self.n_synthetic = 5000\n",
    "        self.output_dir = \"./outputs\"\n",
    "\n",
    "# Define scenarios and n_samples to iterate over\n",
    "scenarios = [\"source_only\", \"fine_tune\", \"traditional_aug\", \"gan_aug\"]\n",
    "n_samples_values = [10, 100, 1000] # Example values, can be adjusted\n",
    "\n",
    "for n_samples_val in n_samples_values:\n",
    "    print(f\"\\n--- Preparing for experiments with N_samples = {n_samples_val} ---\")\n",
    "    # Create a base directory for the current n_samples_val run\n",
    "    base_experiment_run_dir = os.path.join(\"./experiments\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + f\"_N{n_samples_val}\")\n",
    "    os.makedirs(base_experiment_run_dir, exist_ok=True)\n",
    "    print(f\"Experiment Loop: base_experiment_run_dir={base_experiment_run_dir}\")\n",
    "\n",
    "    # Train GAN once for the current n_samples_val if gan_aug is in scenarios\n",
    "    if \"gan_aug\" in scenarios:\n",
    "        print(f\"Experiment Loop: Calling train_gan with output_dir={base_experiment_run_dir}\")\n",
    "        train_gan(\n",
    "            n_samples=n_samples_val,\n",
    "            num_epochs=100, # Using a default for GAN training epochs in notebook\n",
    "            nz=100,\n",
    "            lr=0.0002, # Using default GAN LR\n",
    "            beta1=0.5,\n",
    "            batch_size=64,\n",
    "            ngf=64,\n",
    "            ndf=64,\n",
    "            nc=1,\n",
    "            n_classes=10,\n",
    "            output_dir=base_experiment_run_dir, # Save GAN model to the base run directory\n",
    "        )\n",
    "        print(f\"GAN training for n_samples={n_samples_val} completed.\")\n",
    "\n",
    "    for scenario_name in scenarios:\n",
    "        print(f\"\\nRunning experiment for scenario: {scenario_name} with N={n_samples_val}\")\n",
    "        args = Args()\n",
    "        args.scenario = scenario_name\n",
    "        args.n_samples = n_samples_val\n",
    "        # Create a unique output directory for each experiment run, nested under the base run directory\n",
    "        args.output_dir = os.path.join(base_experiment_run_dir, f\"exp_{scenario_name}_N{n_samples_val}\")\n",
    "\n",
    "        # Input Validation (can be moved to a helper function if desired)\n",
    "        if args.n_samples < 0:\n",
    "            raise ValueError(\"n_samples must be a non-negative integer.\")\n",
    "        if args.batch_size <= 0:\n",
    "            raise ValueError(\"batch_size must be a positive integer.\")\n",
    "        if args.classifier_lr <= 0:\n",
    "            raise ValueError(\"classifier_lr (learning rate) must be a positive float.\")\n",
    "        if args.classifier_epochs_mnist <= 0:\n",
    "            raise ValueError(\"classifier_epochs_mnist must be a positive integer.\")\n",
    "        if args.classifier_epochs_finetune <= 0:\n",
    "            raise ValueError(\"classifier_epochs_finetune must be a positive integer.\")\n",
    "        if args.gan_nz <= 0:\n",
    "            raise ValueError(\"gan_nz (latent vector size) must be a positive integer.\")\n",
    "        if args.gan_ngf <= 0:\n",
    "            raise ValueError(\"gan_ngf (generator feature map size) must be a positive integer.\")\n",
    "        if args.gan_ndf <= 0:\n",
    "            raise ValueError(\"gan_ndf (discriminator feature map size) must be a positive integer.\")\n",
    "        if args.gan_nc <= 0:\n",
    "            raise ValueError(\"gan_nc (number of channels) must be a positive integer.\")\n",
    "        if args.gan_n_classes <= 0:\n",
    "            raise ValueError(\"gan_n_classes (number of classes) must be a positive integer.\")\n",
    "        if args.n_synthetic < 0:\n",
    "            raise ValueError(\"n_synthetic must be a non-negative integer.\")\n",
    "\n",
    "        print(f\"Experiment Loop: Calling run_experiment for scenario={scenario_name}, n_samples={n_samples_val} with args.output_dir={args.output_dir}, gan_model_base_dir={base_experiment_run_dir}\")\n",
    "        run_experiment(args, gan_model_base_dir=base_experiment_run_dir)\n",
    "\n",
    "print(\"\\nAll experiments completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
