Experiment Run Timestamp: 2025-10-22T09:57:46 (Overall experiment start time)
N_Samples: 1000 (for all sub-experiments)
Batch Size: 64 (for all sub-experiments)
Classifier Learning Rate: 0.001 (for all sub-experiments)
Classifier Epochs (MNIST): 5 (for all sub-experiments)
Classifier Epochs (Finetune): 20 (for all sub-experiments)
GAN Parameters:

gan_nz: 100
gan_ngf: 64
gan_ndf: 64
gan_nc: 1
gan_n_classes: 10
n_synthetic: 5000
1. Source Only Scenario (exp_source_only_N1000)

Scenario: source_only
Final Accuracy: 0.2642
Classification Report:
              precision    recall  f1-score   support

           0     0.2886    0.1233    0.1728      1744
           1     0.2970    0.3671    0.3283      5099
           2     0.3053    0.4348    0.3588      4149
           3     0.3236    0.1954    0.2436      2882
           4     0.1260    0.2477    0.1670      2523
           5     0.3302    0.3586    0.3439      2384
           6     0.2632    0.0683    0.1084      1977
           7     0.2819    0.2189    0.2464      2019
           8     0.1948    0.1657    0.1790      1660
           9     0.3140    0.0577    0.0975      1595

    accuracy                         0.2642     26032
   macro avg     0.2725    0.2237    0.2246     26032
weighted avg     0.2780    0.2642    0.2525     26032

Timestamp: 2025-10-22T09:59:22.943582
Log Directory: ./experiments\20251022-095746_N1000\exp_source_only_N1000\tensorboard_logs\20251022-095818_source_only_N1000
2. Fine-Tune Scenario (exp_fine_tune_N1000)

Scenario: fine_tune
Final Accuracy: 0.7208
Classification Report:
              precision    recall  f1-score   support

           0     0.5183    0.8446    0.6424      1744
           1     0.9064    0.7615    0.8277      5099
           2     0.8882    0.7604    0.8194      4149
           3     0.7802    0.4768    0.5919      2882
           4     0.7613    0.7547    0.7580      2523
           5     0.6744    0.7517    0.7110      2384
           6     0.6795    0.6808    0.6801      1977
           7     0.7248    0.8455    0.7805      2019
           8     0.6051    0.5620    0.5828      1660
           9     0.4685    0.7511    0.5771      1595

    accuracy                         0.7208     26032
   macro avg     0.7007    0.7189    0.6971     26032
weighted avg     0.7509    0.7208    0.7246     26032

Timestamp: 2025-10-22T10:00:28.799245
Log Directory: ./experiments\20251022-095746_N1000\exp_fine_tune_N1000\tensorboard_logs\20251022-095922_fine_tune_N1000
3. GAN Augmentation Scenario (exp_gan_aug_N1000)

Scenario: gan_aug
Final Accuracy: 0.6784
Classification Report:
              precision    recall  f1-score   support

           0     0.6905    0.7202    0.7050      1744
           1     0.8892    0.6403    0.7445      5099
           2     0.7740    0.8106    0.7919      4149
           3     0.6173    0.5461    0.5795      2882
           4     0.7879    0.6698    0.7241      2523
           5     0.6495    0.6678    0.6585      2384
           6     0.7086    0.5438    0.6153      1977
           7     0.5811    0.8375    0.6861      2019
           8     0.4343    0.6229    0.5118      1660
           9     0.5000    0.7028    0.5843      1595

    accuracy                         0.6784     26032
   macro avg     0.6632    0.6762    0.6601     26032
weighted avg     0.7052    0.6784    0.6823     26032

Timestamp: 2025-10-22T10:02:47.148365
Log Directory: ./experiments\20251022-095746_N1000\exp_gan_aug_N1000\tensorboard_logs\20251022-100136_gan_aug_N1000
4. Traditional Augmentation Scenario (exp_traditional_aug_N1000)

Scenario: traditional_aug
Final Accuracy: 0.7695
Classification Report:
              precision    recall  f1-score   support

           0     0.6687    0.8354    0.7428      1744
           1     0.8981    0.8211    0.8579      5099
           2     0.9411    0.7746    0.8498      4149
           3     0.7733    0.6995    0.7346      2882
           4     0.6789    0.8664    0.7613      2523
           5     0.8481    0.7118    0.7740      2384
           6     0.6757    0.6566    0.6660      1977
           7     0.7688    0.8400    0.8028      2019
           8     0.5658    0.6090    0.5866      1660
           9     0.6239    0.7956    0.6994      1595

    accuracy                         0.7695     26032
   macro avg     0.7442    0.7610    0.7475     26032
weighted avg     0.7850    0.7695    0.7723     26032

Timestamp: 2025-10-22T10:01:36.349221
Log Directory: ./experiments\20251022-095746_N1000\exp_traditional_aug_N1000\tensorboard_logs\20251022-100028_traditional_aug_N1000
Summary of Accuracies for N=1000:

Source Only: 26.42%
GAN Augmentation: 67.84%
Fine-Tune: 72.08%
Traditional Augmentation: 76.95%
This suite provides the key metrics and classification reports for each scenario run in the latest experiment.

Understanding the Metrics:

Final Accuracy: This is the most straightforward metric, representing the overall percentage of correctly classified samples on the target domain test set. A higher value indicates better performance.
Classification Report: This provides a more granular view of the model's performance for each individual class (digits 0-9).
Precision: For a given class, precision tells you how many of the samples predicted as that class actually belong to that class. High precision means fewer false positives (e.g., predicting a '3' when it was actually a '5').
Recall: For a given class, recall tells you how many of the actual samples of that class were correctly identified. High recall means fewer false negatives (e.g., failing to identify a '3' when it was actually a '3').
F1-score: The F1-score is the harmonic mean of precision and recall. It's particularly useful when you need to balance both metrics, especially if there's an uneven class distribution.
Support: This indicates the number of true instances for each class in the test set.
Macro Avg: This is the unweighted average of precision, recall, and F1-score across all classes. It treats all classes equally, regardless of their size.
Weighted Avg: This is the average of precision, recall, and F1-score across all classes, weighted by the number of samples in each class (support). This is often more representative when class distributions are imbalanced.
Interpreting the N=1000 Scenario Comparisons:

Source Only (26.42% Accuracy): This is your baseline. It represents a model trained exclusively on the source domain (e.g., MNIST) and then directly applied to the target domain (e.g., SVHN). The low accuracy here highlights the significant "domain shift" â€“ the difference in data distribution between the source and target domains. Without any adaptation, the model struggles to generalize.
GAN Augmentation (67.84% Accuracy): This scenario uses a Generative Adversarial Network (GAN) to create synthetic target-like images, which are then added to the training data. The substantial increase in accuracy compared to "Source Only" demonstrates that generating and incorporating synthetic data can effectively help the model adapt to the new domain.
Fine-Tune (72.08% Accuracy): Here, a model pre-trained on the source domain is further trained (fine-tuned) using a small amount of actual labeled data from the target domain. The improved performance over "GAN Augmentation" suggests that even a limited amount of real target domain data can be very powerful for adaptation, allowing the model to adjust its learned features to the new domain's specifics.
Traditional Augmentation (76.95% Accuracy): This approach applies standard image augmentation techniques (like rotations, flips, shifts) to the existing training data. In this specific experiment, traditional augmentation yielded the highest accuracy. This indicates that for this particular domain adaptation task and dataset, simple, well-understood augmentation methods were highly effective in improving generalization to the target domain.
In summary, for N=1000, traditional augmentation performed the best, followed by fine-tuning, then GAN augmentation, with source-only being the weakest. This suggests that while GANs are helpful, direct exposure to target data (fine-tuning) or robust traditional augmentation can be more effective depending on the task and data characteristics.

[MEMORY BANK: INACTIVE]
Here is a comprehensive result suite for the latest run, including interpretation across different sample sizes (N).

Experiment Run Timestamp: 2025-10-22 (Overall experiment start date)
Common Parameters:

Batch Size: 64
Classifier Learning Rate: 0.001
Classifier Epochs (MNIST): 5
Classifier Epochs (Finetune): 20
GAN Parameters: gan_nz: 100, gan_ngf: 64, gan_ndf: 64, gan_nc: 1, gan_n_classes: 10
Number of Synthetic Samples (n_synthetic): 5000
Interpretation of Results
The results demonstrate the impact of different domain adaptation strategies and the number of labeled target samples (n_samples) on classification accuracy.

Source Only: This serves as the baseline, showing performance without any domain adaptation. As expected, accuracy is generally low due to the domain shift.
Fine-Tune: This strategy involves pre-training on the source domain and then fine-tuning the model with a small number of labeled target samples. It's often very effective, especially when target data is scarce.
GAN Augmentation: This method uses a GAN to generate synthetic target-like images to augment the training data. It aims to improve generalization by increasing the diversity of the training set.
Traditional Augmentation: This involves applying standard image transformations (e.g., rotations, flips) to the existing training data to create more variations.
Key Observations:

Impact of n_samples: Generally, as n_samples increases, the performance of all adaptation methods tends to improve, as the model has more target-specific information to learn from.
Fine-Tuning vs. Augmentation:
For very small n_samples (e.g., N=10), fine-tuning can sometimes struggle or even perform worse than source-only if the small target dataset is not representative or leads to overfitting. GAN augmentation might offer a more stable improvement in such cases by providing diverse synthetic data.
As n_samples increases, fine-tuning typically becomes very effective, often outperforming augmentation methods because it directly leverages real target domain labels.
Traditional vs. GAN Augmentation: The effectiveness of traditional vs. GAN augmentation can vary. Traditional methods are simpler and often robust, while GANs can generate more complex, domain-specific variations but are also harder to train and can sometimes produce less useful samples.
Detailed Results by n_samples
1. Results for N=10 Samples (experiments/20251022-094835_N10/)
Scenario: source_only
Final Accuracy: 0.2667
Timestamp: 2025-10-22T09:49:51.842751
Scenario: fine_tune
Final Accuracy: 0.1013
Timestamp: 2025-10-22T09:50:58.114328
Interpretation: For N=10, fine-tuning performed worse than source-only. This suggests that with extremely limited target data, fine-tuning might overfit to the few samples or struggle to adapt effectively, leading to a degradation in performance compared to a model that hasn't seen any target data.
Scenario: gan_aug
Final Accuracy: 0.1718
Timestamp: 2025-10-22T09:53:11.809254
Interpretation: GAN augmentation provided a slight improvement over fine-tuning for N=10, but still significantly lower than source-only. This indicates that while synthetic data can help, the quantity and quality of generated samples might not be sufficient to overcome the extreme data scarcity.
Scenario: traditional_aug
Final Accuracy: 0.1656
Timestamp: 2025-10-22T09:52:01.984050
Interpretation: Similar to GAN augmentation, traditional augmentation also performed worse than source-only for N=10. This reinforces the challenge of domain adaptation with very few labeled target samples, where even basic augmentation might not be enough to bridge the domain gap effectively.
Summary of Accuracies for N=10:

Source Only: 26.67%
Fine-Tune: 10.13%
GAN Augmentation: 17.18%
Traditional Augmentation: 16.56%
2. Results for N=100 Samples (experiments/20251022-095311_N100/)
Scenario: source_only
Final Accuracy: 0.2948
Timestamp: 2025-10-22T09:54:29.116648
Scenario: fine_tune
Final Accuracy: 0.4169
Timestamp: 2025-10-22T09:55:34.813163
Interpretation: With N=100, fine-tuning shows a significant improvement over source-only, indicating that a moderate amount of real target data is beneficial for adapting the pre-trained model.
Scenario: gan_aug
Final Accuracy: 0.3649
Timestamp: 2025-10-22T09:57:46.857525
Interpretation: GAN augmentation also improves upon source-only, but fine-tuning is more effective. This suggests that while synthetic data helps, the direct signal from real target labels in fine-tuning is more potent at this sample size.
Scenario: traditional_aug
Final Accuracy: 0.3476
Timestamp: 2025-10-22T09:56:38.880636
Interpretation: Traditional augmentation provides an improvement over source-only, but is less effective than fine-tuning. This indicates that for N=100, leveraging real target data directly (fine-tuning) is more impactful than relying solely on augmented data.
Summary of Accuracies for N=100:

Source Only: 29.48%
Fine-Tune: 41.69%
GAN Augmentation: 36.49%
Traditional Augmentation: 34.76%
3. Results for N=1000 Samples (experiments/20251022-095746_N1000/)
Scenario: source_only
Final Accuracy: 0.2642
Timestamp: 2025-10-22T09:59:22.943582
Scenario: fine_tune
Final Accuracy: 0.7208
Timestamp: 2025-10-22T10:00:28.799245
Interpretation: Fine-tuning shows a very strong performance, significantly outperforming source-only. With a larger number of target samples, the model can effectively adapt its features to the target domain.
Scenario: gan_aug
Final Accuracy: 0.6784
Timestamp: 2025-10-22T10:02:47.148365
Interpretation: GAN augmentation provides a substantial boost over source-only, demonstrating its effectiveness when more target-like synthetic data can be generated and utilized.
Scenario: traditional_aug
Final Accuracy: 0.7695
Timestamp: 2025-10-22T10:01:36.349221
Interpretation: Traditional augmentation achieved the highest accuracy for N=1000. This suggests that for this specific dataset and task, simple yet effective transformations on a larger set of target samples can lead to superior generalization.
Summary of Accuracies for N=1000:

Source Only: 26.42%
Fine-Tune: 72.08%
GAN Augmentation: 67.84%
Traditional Augmentation: 76.95%
This comprehensive suite provides a clear overview of the experiment results, highlighting the performance of different domain adaptation strategies across varying amounts of labeled target data.